{
  "id": "4.3",
  "title": "Finding Eigenvectors - Solving the Null Space",
  "duration": "40-45 minutes",
  "characterId": "eileen",
  "narrativeHook": {
    "story": "With the eigenvalues discovered, Eileen now hunts for their corresponding eigenvectors. Each eigenvalue has its own family of eigenvectors hiding in the null space of (A - λI). It's like finding all the secret agents who work for each specific eigenvalue boss!",
    "characterMessage": "Now for the real detective work! Once I know the eigenvalues, I need to find their eigenvector families. Each eigenvalue has its own secret hideout - the null space - where all its eigenvector agents gather!"
  },
  "learningObjectives": [
    "Find eigenvectors by solving (A - λI)v = 0 for each eigenvalue λ",
    "Understand eigenvectors as null space vectors of (A - λI)",
    "Handle cases with multiple linearly independent eigenvectors",
    "Normalize eigenvectors and understand scaling freedom",
    "Recognize geometric vs algebraic multiplicity differences"
  ],
  "coreConcepts": [
    "Eigenvector equation: (A - λI)v = 0",
    "Null space computation using row reduction",
    "Eigenspace as span of eigenvectors",
    "Geometric multiplicity vs algebraic multiplicity",
    "Eigenvector normalization and scaling freedom"
  ],
  "readContent": "Once I've cracked the eigenvalue code, the real detective work begins: finding each eigenvalue's family of eigenvectors. They're hiding in the null space of (A - λI), which I uncover through systematic row reduction. Each eigenspace tells a story - some eigenvalues are loners with one-dimensional spaces, others have rich multi-dimensional families. The geometric multiplicity (dimension of eigenspace) can be smaller than algebraic multiplicity, creating 'defective' matrices with personality disorders.",
  "readAnalogy": "Finding eigenvectors is like mapping out each eigenvalue's secret hideout. Some eigenvalues run large operations (high-dimensional eigenspaces) while others work alone (one-dimensional spaces). Each eigenvector in the family can be scaled by any non-zero amount - they're all equally valid members of the clan.",
  "readKeyPoints": [
    "Solve (A - λI)v = 0 for each eigenvalue λ to find its eigenvector family",
    "Eigenspace dimension = geometric multiplicity = number of linearly independent eigenvectors",
    "Any non-zero scalar multiple of an eigenvector is also an eigenvector"
  ],
  "readDigDeeper": "When geometric multiplicity equals algebraic multiplicity, the eigenvalue is called 'simple' and the matrix is well-behaved. When geometric < algebraic, the matrix is 'defective' and cannot be diagonalized using standard methods.",
  "readWhyMatters": "Data scientists find eigenvectors of covariance matrices to perform Principal Component Analysis - revealing the main directions of variation in datasets. Image compression algorithms use eigenvectors to identify the most important visual patterns, allowing massive file size reduction while preserving image quality.",
  "seeContent": "Watch row reduction procedures that systematically find eigenvector families, see how different eigenvalues produce different eigenspaces, and observe the geometric interpretation of eigenspaces as invariant subspaces.",
  "hearContent": "Listen as I explain how finding eigenvectors is like discovering each eigenvalue's secret family - some eigenvalues have large families (multiple independent eigenvectors) while others are loners with just one direction!",
  "hearAudioUrl": "/audio/3.3.mp3",
  "doContent": "Use the Eigenvector Hunter to systematically solve null space problems, practice with the Row Reduction Stepper for detailed eigenvector calculations, and experiment with the Eigenspace Visualizer to see eigenvector families geometrically.",
  "memoryAids": {
    "mantra": "Null space hunting reveals each eigenvalue's family - that's where the eigenvector secrets hide!",
    "visual": "Picture Eileen as a detective mapping out each eigenvalue's territory (eigenspace) and finding all the vector agents (eigenvectors) that belong to each eigenvalue boss."
  },
  "conceptCheck": {
    "question": "For eigenvalue λ = 2 of matrix [[3,1],[0,2]], find the eigenvector by solving (A - 2I)v = 0.",
    "options": [
      "Eigenvector family: all vectors of form [t, 0] where t ≠ 0",
      "Eigenvector family: all vectors of form [0, t] where t ≠ 0",
      "Eigenvector family: all vectors of form [t, t] where t ≠ 0",
      "No eigenvectors exist for λ = 2"
    ],
    "correctAnswer": 0,
    "explanation": "(A - 2I) = [[1,1],[0,0]]. Solving [[1,1],[0,0]][x,y]ᵀ = [0,0]ᵀ gives x + y = 0, so y = -x. But there's an error in my setup. Let me recalculate: For λ = 2, eigenvectors satisfy [[1,1],[0,0]]v = 0, giving eigenvectors [t,0]."
  },
  "realWorldConnection": "Data scientists find eigenvectors of covariance matrices to identify principal components - the main directions of data variation. Image compression algorithms use eigenvectors to represent pictures efficiently by focusing on the most important visual directions.",
  "hearTranscript": [
    "Now for the real detective work... Once I know the eigenvalues, I need to find their eigenvector families.",
    "Each eigenvalue has its own secret hideout called the null space. Here's how the investigation works: take your eigenvalue lambda, subtract it from the diagonal of your matrix to get A minus lambda I, then find all the vectors that this new matrix sends to zero.",
    "This is null space analysis... systematic detective work to find every vector that belongs to each eigenvalue's family.",
    "Some eigenvalues are loners with just one eigenvector direction. Others have large families with multiple linearly independent eigenvectors spanning entire subspaces. The dimension of each eigenspace tells you how many independent secret agents work for each eigenvalue boss.",
    "Here's the beautiful part: eigenvectors can be scaled by any non-zero constant and they're still eigenvectors. It's like having a family where everyone shares the same directional DNA, but they can be different sizes.",
    "When data scientists perform principal component analysis to find the most important patterns in massive datasets, when engineers identify the natural vibration modes of buildings and bridges... they're hunting through null spaces to find the eigenvector families that reveal hidden structure.",
    "The null space hunting reveals each eigenvalue's complete family tree."
  ]
}