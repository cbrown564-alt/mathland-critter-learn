
<!doctype html>
<html lang="en">

<head>
    <title>Code coverage report for src/utils/lessonData/module8.ts</title>
    <meta charset="utf-8" />
    <link rel="stylesheet" href="../../../prettify.css" />
    <link rel="stylesheet" href="../../../base.css" />
    <link rel="shortcut icon" type="image/x-icon" href="../../../favicon.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style type='text/css'>
        .coverage-summary .sorter {
            background-image: url(../../../sort-arrow-sprite.png);
        }
    </style>
</head>
    
<body>
<div class='wrapper'>
    <div class='pad1'>
        <h1><a href="../../../index.html">All files</a> / <a href="index.html">src/utils/lessonData</a> module8.ts</h1>
        <div class='clearfix'>
            
            <div class='fl pad1y space-right2'>
                <span class="strong">100% </span>
                <span class="quiet">Statements</span>
                <span class='fraction'>1/1</span>
            </div>
        
            
            <div class='fl pad1y space-right2'>
                <span class="strong">100% </span>
                <span class="quiet">Branches</span>
                <span class='fraction'>0/0</span>
            </div>
        
            
            <div class='fl pad1y space-right2'>
                <span class="strong">100% </span>
                <span class="quiet">Functions</span>
                <span class='fraction'>0/0</span>
            </div>
        
            
            <div class='fl pad1y space-right2'>
                <span class="strong">100% </span>
                <span class="quiet">Lines</span>
                <span class='fraction'>1/1</span>
            </div>
        
            
        </div>
        <p class="quiet">
            Press <em>n</em> or <em>j</em> to go to the next uncovered block, <em>b</em>, <em>p</em> or <em>k</em> for the previous block.
        </p>
        <template id="filterTemplate">
            <div class="quiet">
                Filter:
                <input type="search" id="fileSearch">
            </div>
        </template>
    </div>
    <div class='status-line high'></div>
    <pre><table class="coverage">
<tr><td class="line-count quiet"><a name='L1'></a><a href='#L1'>1</a>
<a name='L2'></a><a href='#L2'>2</a>
<a name='L3'></a><a href='#L3'>3</a>
<a name='L4'></a><a href='#L4'>4</a>
<a name='L5'></a><a href='#L5'>5</a>
<a name='L6'></a><a href='#L6'>6</a>
<a name='L7'></a><a href='#L7'>7</a>
<a name='L8'></a><a href='#L8'>8</a>
<a name='L9'></a><a href='#L9'>9</a>
<a name='L10'></a><a href='#L10'>10</a>
<a name='L11'></a><a href='#L11'>11</a>
<a name='L12'></a><a href='#L12'>12</a>
<a name='L13'></a><a href='#L13'>13</a>
<a name='L14'></a><a href='#L14'>14</a>
<a name='L15'></a><a href='#L15'>15</a>
<a name='L16'></a><a href='#L16'>16</a>
<a name='L17'></a><a href='#L17'>17</a>
<a name='L18'></a><a href='#L18'>18</a>
<a name='L19'></a><a href='#L19'>19</a>
<a name='L20'></a><a href='#L20'>20</a>
<a name='L21'></a><a href='#L21'>21</a>
<a name='L22'></a><a href='#L22'>22</a>
<a name='L23'></a><a href='#L23'>23</a>
<a name='L24'></a><a href='#L24'>24</a>
<a name='L25'></a><a href='#L25'>25</a>
<a name='L26'></a><a href='#L26'>26</a>
<a name='L27'></a><a href='#L27'>27</a>
<a name='L28'></a><a href='#L28'>28</a>
<a name='L29'></a><a href='#L29'>29</a>
<a name='L30'></a><a href='#L30'>30</a>
<a name='L31'></a><a href='#L31'>31</a>
<a name='L32'></a><a href='#L32'>32</a>
<a name='L33'></a><a href='#L33'>33</a>
<a name='L34'></a><a href='#L34'>34</a>
<a name='L35'></a><a href='#L35'>35</a>
<a name='L36'></a><a href='#L36'>36</a>
<a name='L37'></a><a href='#L37'>37</a>
<a name='L38'></a><a href='#L38'>38</a>
<a name='L39'></a><a href='#L39'>39</a>
<a name='L40'></a><a href='#L40'>40</a>
<a name='L41'></a><a href='#L41'>41</a>
<a name='L42'></a><a href='#L42'>42</a>
<a name='L43'></a><a href='#L43'>43</a>
<a name='L44'></a><a href='#L44'>44</a>
<a name='L45'></a><a href='#L45'>45</a>
<a name='L46'></a><a href='#L46'>46</a>
<a name='L47'></a><a href='#L47'>47</a>
<a name='L48'></a><a href='#L48'>48</a>
<a name='L49'></a><a href='#L49'>49</a>
<a name='L50'></a><a href='#L50'>50</a>
<a name='L51'></a><a href='#L51'>51</a>
<a name='L52'></a><a href='#L52'>52</a>
<a name='L53'></a><a href='#L53'>53</a>
<a name='L54'></a><a href='#L54'>54</a>
<a name='L55'></a><a href='#L55'>55</a>
<a name='L56'></a><a href='#L56'>56</a>
<a name='L57'></a><a href='#L57'>57</a>
<a name='L58'></a><a href='#L58'>58</a>
<a name='L59'></a><a href='#L59'>59</a>
<a name='L60'></a><a href='#L60'>60</a>
<a name='L61'></a><a href='#L61'>61</a>
<a name='L62'></a><a href='#L62'>62</a>
<a name='L63'></a><a href='#L63'>63</a>
<a name='L64'></a><a href='#L64'>64</a>
<a name='L65'></a><a href='#L65'>65</a>
<a name='L66'></a><a href='#L66'>66</a>
<a name='L67'></a><a href='#L67'>67</a>
<a name='L68'></a><a href='#L68'>68</a>
<a name='L69'></a><a href='#L69'>69</a>
<a name='L70'></a><a href='#L70'>70</a>
<a name='L71'></a><a href='#L71'>71</a>
<a name='L72'></a><a href='#L72'>72</a>
<a name='L73'></a><a href='#L73'>73</a>
<a name='L74'></a><a href='#L74'>74</a>
<a name='L75'></a><a href='#L75'>75</a>
<a name='L76'></a><a href='#L76'>76</a>
<a name='L77'></a><a href='#L77'>77</a>
<a name='L78'></a><a href='#L78'>78</a>
<a name='L79'></a><a href='#L79'>79</a>
<a name='L80'></a><a href='#L80'>80</a>
<a name='L81'></a><a href='#L81'>81</a>
<a name='L82'></a><a href='#L82'>82</a>
<a name='L83'></a><a href='#L83'>83</a>
<a name='L84'></a><a href='#L84'>84</a>
<a name='L85'></a><a href='#L85'>85</a>
<a name='L86'></a><a href='#L86'>86</a>
<a name='L87'></a><a href='#L87'>87</a>
<a name='L88'></a><a href='#L88'>88</a>
<a name='L89'></a><a href='#L89'>89</a>
<a name='L90'></a><a href='#L90'>90</a>
<a name='L91'></a><a href='#L91'>91</a>
<a name='L92'></a><a href='#L92'>92</a>
<a name='L93'></a><a href='#L93'>93</a>
<a name='L94'></a><a href='#L94'>94</a>
<a name='L95'></a><a href='#L95'>95</a>
<a name='L96'></a><a href='#L96'>96</a>
<a name='L97'></a><a href='#L97'>97</a>
<a name='L98'></a><a href='#L98'>98</a>
<a name='L99'></a><a href='#L99'>99</a>
<a name='L100'></a><a href='#L100'>100</a>
<a name='L101'></a><a href='#L101'>101</a>
<a name='L102'></a><a href='#L102'>102</a>
<a name='L103'></a><a href='#L103'>103</a>
<a name='L104'></a><a href='#L104'>104</a>
<a name='L105'></a><a href='#L105'>105</a>
<a name='L106'></a><a href='#L106'>106</a>
<a name='L107'></a><a href='#L107'>107</a>
<a name='L108'></a><a href='#L108'>108</a>
<a name='L109'></a><a href='#L109'>109</a>
<a name='L110'></a><a href='#L110'>110</a>
<a name='L111'></a><a href='#L111'>111</a>
<a name='L112'></a><a href='#L112'>112</a>
<a name='L113'></a><a href='#L113'>113</a>
<a name='L114'></a><a href='#L114'>114</a>
<a name='L115'></a><a href='#L115'>115</a>
<a name='L116'></a><a href='#L116'>116</a>
<a name='L117'></a><a href='#L117'>117</a>
<a name='L118'></a><a href='#L118'>118</a>
<a name='L119'></a><a href='#L119'>119</a>
<a name='L120'></a><a href='#L120'>120</a>
<a name='L121'></a><a href='#L121'>121</a>
<a name='L122'></a><a href='#L122'>122</a>
<a name='L123'></a><a href='#L123'>123</a>
<a name='L124'></a><a href='#L124'>124</a>
<a name='L125'></a><a href='#L125'>125</a>
<a name='L126'></a><a href='#L126'>126</a>
<a name='L127'></a><a href='#L127'>127</a>
<a name='L128'></a><a href='#L128'>128</a>
<a name='L129'></a><a href='#L129'>129</a>
<a name='L130'></a><a href='#L130'>130</a>
<a name='L131'></a><a href='#L131'>131</a>
<a name='L132'></a><a href='#L132'>132</a>
<a name='L133'></a><a href='#L133'>133</a>
<a name='L134'></a><a href='#L134'>134</a>
<a name='L135'></a><a href='#L135'>135</a>
<a name='L136'></a><a href='#L136'>136</a>
<a name='L137'></a><a href='#L137'>137</a>
<a name='L138'></a><a href='#L138'>138</a>
<a name='L139'></a><a href='#L139'>139</a>
<a name='L140'></a><a href='#L140'>140</a>
<a name='L141'></a><a href='#L141'>141</a>
<a name='L142'></a><a href='#L142'>142</a>
<a name='L143'></a><a href='#L143'>143</a>
<a name='L144'></a><a href='#L144'>144</a>
<a name='L145'></a><a href='#L145'>145</a>
<a name='L146'></a><a href='#L146'>146</a>
<a name='L147'></a><a href='#L147'>147</a>
<a name='L148'></a><a href='#L148'>148</a>
<a name='L149'></a><a href='#L149'>149</a>
<a name='L150'></a><a href='#L150'>150</a>
<a name='L151'></a><a href='#L151'>151</a>
<a name='L152'></a><a href='#L152'>152</a>
<a name='L153'></a><a href='#L153'>153</a>
<a name='L154'></a><a href='#L154'>154</a>
<a name='L155'></a><a href='#L155'>155</a>
<a name='L156'></a><a href='#L156'>156</a>
<a name='L157'></a><a href='#L157'>157</a>
<a name='L158'></a><a href='#L158'>158</a>
<a name='L159'></a><a href='#L159'>159</a>
<a name='L160'></a><a href='#L160'>160</a>
<a name='L161'></a><a href='#L161'>161</a>
<a name='L162'></a><a href='#L162'>162</a>
<a name='L163'></a><a href='#L163'>163</a>
<a name='L164'></a><a href='#L164'>164</a>
<a name='L165'></a><a href='#L165'>165</a>
<a name='L166'></a><a href='#L166'>166</a>
<a name='L167'></a><a href='#L167'>167</a>
<a name='L168'></a><a href='#L168'>168</a>
<a name='L169'></a><a href='#L169'>169</a>
<a name='L170'></a><a href='#L170'>170</a>
<a name='L171'></a><a href='#L171'>171</a>
<a name='L172'></a><a href='#L172'>172</a>
<a name='L173'></a><a href='#L173'>173</a>
<a name='L174'></a><a href='#L174'>174</a>
<a name='L175'></a><a href='#L175'>175</a>
<a name='L176'></a><a href='#L176'>176</a>
<a name='L177'></a><a href='#L177'>177</a>
<a name='L178'></a><a href='#L178'>178</a>
<a name='L179'></a><a href='#L179'>179</a>
<a name='L180'></a><a href='#L180'>180</a>
<a name='L181'></a><a href='#L181'>181</a>
<a name='L182'></a><a href='#L182'>182</a>
<a name='L183'></a><a href='#L183'>183</a>
<a name='L184'></a><a href='#L184'>184</a>
<a name='L185'></a><a href='#L185'>185</a>
<a name='L186'></a><a href='#L186'>186</a>
<a name='L187'></a><a href='#L187'>187</a>
<a name='L188'></a><a href='#L188'>188</a>
<a name='L189'></a><a href='#L189'>189</a>
<a name='L190'></a><a href='#L190'>190</a>
<a name='L191'></a><a href='#L191'>191</a>
<a name='L192'></a><a href='#L192'>192</a>
<a name='L193'></a><a href='#L193'>193</a>
<a name='L194'></a><a href='#L194'>194</a>
<a name='L195'></a><a href='#L195'>195</a>
<a name='L196'></a><a href='#L196'>196</a>
<a name='L197'></a><a href='#L197'>197</a>
<a name='L198'></a><a href='#L198'>198</a>
<a name='L199'></a><a href='#L199'>199</a>
<a name='L200'></a><a href='#L200'>200</a>
<a name='L201'></a><a href='#L201'>201</a>
<a name='L202'></a><a href='#L202'>202</a>
<a name='L203'></a><a href='#L203'>203</a>
<a name='L204'></a><a href='#L204'>204</a>
<a name='L205'></a><a href='#L205'>205</a>
<a name='L206'></a><a href='#L206'>206</a>
<a name='L207'></a><a href='#L207'>207</a>
<a name='L208'></a><a href='#L208'>208</a>
<a name='L209'></a><a href='#L209'>209</a>
<a name='L210'></a><a href='#L210'>210</a>
<a name='L211'></a><a href='#L211'>211</a>
<a name='L212'></a><a href='#L212'>212</a>
<a name='L213'></a><a href='#L213'>213</a>
<a name='L214'></a><a href='#L214'>214</a>
<a name='L215'></a><a href='#L215'>215</a>
<a name='L216'></a><a href='#L216'>216</a>
<a name='L217'></a><a href='#L217'>217</a>
<a name='L218'></a><a href='#L218'>218</a>
<a name='L219'></a><a href='#L219'>219</a>
<a name='L220'></a><a href='#L220'>220</a>
<a name='L221'></a><a href='#L221'>221</a>
<a name='L222'></a><a href='#L222'>222</a>
<a name='L223'></a><a href='#L223'>223</a>
<a name='L224'></a><a href='#L224'>224</a>
<a name='L225'></a><a href='#L225'>225</a>
<a name='L226'></a><a href='#L226'>226</a>
<a name='L227'></a><a href='#L227'>227</a>
<a name='L228'></a><a href='#L228'>228</a>
<a name='L229'></a><a href='#L229'>229</a>
<a name='L230'></a><a href='#L230'>230</a>
<a name='L231'></a><a href='#L231'>231</a>
<a name='L232'></a><a href='#L232'>232</a>
<a name='L233'></a><a href='#L233'>233</a>
<a name='L234'></a><a href='#L234'>234</a>
<a name='L235'></a><a href='#L235'>235</a>
<a name='L236'></a><a href='#L236'>236</a>
<a name='L237'></a><a href='#L237'>237</a>
<a name='L238'></a><a href='#L238'>238</a>
<a name='L239'></a><a href='#L239'>239</a>
<a name='L240'></a><a href='#L240'>240</a>
<a name='L241'></a><a href='#L241'>241</a>
<a name='L242'></a><a href='#L242'>242</a>
<a name='L243'></a><a href='#L243'>243</a>
<a name='L244'></a><a href='#L244'>244</a>
<a name='L245'></a><a href='#L245'>245</a>
<a name='L246'></a><a href='#L246'>246</a>
<a name='L247'></a><a href='#L247'>247</a>
<a name='L248'></a><a href='#L248'>248</a>
<a name='L249'></a><a href='#L249'>249</a>
<a name='L250'></a><a href='#L250'>250</a>
<a name='L251'></a><a href='#L251'>251</a>
<a name='L252'></a><a href='#L252'>252</a>
<a name='L253'></a><a href='#L253'>253</a>
<a name='L254'></a><a href='#L254'>254</a>
<a name='L255'></a><a href='#L255'>255</a>
<a name='L256'></a><a href='#L256'>256</a>
<a name='L257'></a><a href='#L257'>257</a>
<a name='L258'></a><a href='#L258'>258</a>
<a name='L259'></a><a href='#L259'>259</a>
<a name='L260'></a><a href='#L260'>260</a>
<a name='L261'></a><a href='#L261'>261</a>
<a name='L262'></a><a href='#L262'>262</a>
<a name='L263'></a><a href='#L263'>263</a>
<a name='L264'></a><a href='#L264'>264</a>
<a name='L265'></a><a href='#L265'>265</a>
<a name='L266'></a><a href='#L266'>266</a>
<a name='L267'></a><a href='#L267'>267</a>
<a name='L268'></a><a href='#L268'>268</a>
<a name='L269'></a><a href='#L269'>269</a>
<a name='L270'></a><a href='#L270'>270</a>
<a name='L271'></a><a href='#L271'>271</a>
<a name='L272'></a><a href='#L272'>272</a>
<a name='L273'></a><a href='#L273'>273</a>
<a name='L274'></a><a href='#L274'>274</a>
<a name='L275'></a><a href='#L275'>275</a>
<a name='L276'></a><a href='#L276'>276</a>
<a name='L277'></a><a href='#L277'>277</a>
<a name='L278'></a><a href='#L278'>278</a>
<a name='L279'></a><a href='#L279'>279</a>
<a name='L280'></a><a href='#L280'>280</a>
<a name='L281'></a><a href='#L281'>281</a>
<a name='L282'></a><a href='#L282'>282</a>
<a name='L283'></a><a href='#L283'>283</a>
<a name='L284'></a><a href='#L284'>284</a>
<a name='L285'></a><a href='#L285'>285</a>
<a name='L286'></a><a href='#L286'>286</a>
<a name='L287'></a><a href='#L287'>287</a>
<a name='L288'></a><a href='#L288'>288</a>
<a name='L289'></a><a href='#L289'>289</a>
<a name='L290'></a><a href='#L290'>290</a>
<a name='L291'></a><a href='#L291'>291</a>
<a name='L292'></a><a href='#L292'>292</a>
<a name='L293'></a><a href='#L293'>293</a>
<a name='L294'></a><a href='#L294'>294</a>
<a name='L295'></a><a href='#L295'>295</a>
<a name='L296'></a><a href='#L296'>296</a>
<a name='L297'></a><a href='#L297'>297</a>
<a name='L298'></a><a href='#L298'>298</a>
<a name='L299'></a><a href='#L299'>299</a>
<a name='L300'></a><a href='#L300'>300</a>
<a name='L301'></a><a href='#L301'>301</a>
<a name='L302'></a><a href='#L302'>302</a>
<a name='L303'></a><a href='#L303'>303</a>
<a name='L304'></a><a href='#L304'>304</a>
<a name='L305'></a><a href='#L305'>305</a>
<a name='L306'></a><a href='#L306'>306</a>
<a name='L307'></a><a href='#L307'>307</a>
<a name='L308'></a><a href='#L308'>308</a>
<a name='L309'></a><a href='#L309'>309</a>
<a name='L310'></a><a href='#L310'>310</a>
<a name='L311'></a><a href='#L311'>311</a>
<a name='L312'></a><a href='#L312'>312</a>
<a name='L313'></a><a href='#L313'>313</a>
<a name='L314'></a><a href='#L314'>314</a>
<a name='L315'></a><a href='#L315'>315</a>
<a name='L316'></a><a href='#L316'>316</a>
<a name='L317'></a><a href='#L317'>317</a>
<a name='L318'></a><a href='#L318'>318</a>
<a name='L319'></a><a href='#L319'>319</a>
<a name='L320'></a><a href='#L320'>320</a>
<a name='L321'></a><a href='#L321'>321</a>
<a name='L322'></a><a href='#L322'>322</a>
<a name='L323'></a><a href='#L323'>323</a>
<a name='L324'></a><a href='#L324'>324</a>
<a name='L325'></a><a href='#L325'>325</a>
<a name='L326'></a><a href='#L326'>326</a>
<a name='L327'></a><a href='#L327'>327</a>
<a name='L328'></a><a href='#L328'>328</a>
<a name='L329'></a><a href='#L329'>329</a>
<a name='L330'></a><a href='#L330'>330</a>
<a name='L331'></a><a href='#L331'>331</a>
<a name='L332'></a><a href='#L332'>332</a>
<a name='L333'></a><a href='#L333'>333</a>
<a name='L334'></a><a href='#L334'>334</a>
<a name='L335'></a><a href='#L335'>335</a>
<a name='L336'></a><a href='#L336'>336</a>
<a name='L337'></a><a href='#L337'>337</a>
<a name='L338'></a><a href='#L338'>338</a>
<a name='L339'></a><a href='#L339'>339</a>
<a name='L340'></a><a href='#L340'>340</a>
<a name='L341'></a><a href='#L341'>341</a>
<a name='L342'></a><a href='#L342'>342</a>
<a name='L343'></a><a href='#L343'>343</a>
<a name='L344'></a><a href='#L344'>344</a>
<a name='L345'></a><a href='#L345'>345</a>
<a name='L346'></a><a href='#L346'>346</a>
<a name='L347'></a><a href='#L347'>347</a>
<a name='L348'></a><a href='#L348'>348</a>
<a name='L349'></a><a href='#L349'>349</a>
<a name='L350'></a><a href='#L350'>350</a>
<a name='L351'></a><a href='#L351'>351</a>
<a name='L352'></a><a href='#L352'>352</a>
<a name='L353'></a><a href='#L353'>353</a>
<a name='L354'></a><a href='#L354'>354</a>
<a name='L355'></a><a href='#L355'>355</a>
<a name='L356'></a><a href='#L356'>356</a>
<a name='L357'></a><a href='#L357'>357</a>
<a name='L358'></a><a href='#L358'>358</a>
<a name='L359'></a><a href='#L359'>359</a>
<a name='L360'></a><a href='#L360'>360</a>
<a name='L361'></a><a href='#L361'>361</a>
<a name='L362'></a><a href='#L362'>362</a>
<a name='L363'></a><a href='#L363'>363</a>
<a name='L364'></a><a href='#L364'>364</a>
<a name='L365'></a><a href='#L365'>365</a>
<a name='L366'></a><a href='#L366'>366</a>
<a name='L367'></a><a href='#L367'>367</a>
<a name='L368'></a><a href='#L368'>368</a>
<a name='L369'></a><a href='#L369'>369</a>
<a name='L370'></a><a href='#L370'>370</a>
<a name='L371'></a><a href='#L371'>371</a>
<a name='L372'></a><a href='#L372'>372</a>
<a name='L373'></a><a href='#L373'>373</a>
<a name='L374'></a><a href='#L374'>374</a>
<a name='L375'></a><a href='#L375'>375</a>
<a name='L376'></a><a href='#L376'>376</a>
<a name='L377'></a><a href='#L377'>377</a>
<a name='L378'></a><a href='#L378'>378</a>
<a name='L379'></a><a href='#L379'>379</a>
<a name='L380'></a><a href='#L380'>380</a>
<a name='L381'></a><a href='#L381'>381</a>
<a name='L382'></a><a href='#L382'>382</a>
<a name='L383'></a><a href='#L383'>383</a>
<a name='L384'></a><a href='#L384'>384</a>
<a name='L385'></a><a href='#L385'>385</a>
<a name='L386'></a><a href='#L386'>386</a>
<a name='L387'></a><a href='#L387'>387</a>
<a name='L388'></a><a href='#L388'>388</a>
<a name='L389'></a><a href='#L389'>389</a>
<a name='L390'></a><a href='#L390'>390</a>
<a name='L391'></a><a href='#L391'>391</a>
<a name='L392'></a><a href='#L392'>392</a>
<a name='L393'></a><a href='#L393'>393</a>
<a name='L394'></a><a href='#L394'>394</a>
<a name='L395'></a><a href='#L395'>395</a>
<a name='L396'></a><a href='#L396'>396</a>
<a name='L397'></a><a href='#L397'>397</a>
<a name='L398'></a><a href='#L398'>398</a>
<a name='L399'></a><a href='#L399'>399</a>
<a name='L400'></a><a href='#L400'>400</a>
<a name='L401'></a><a href='#L401'>401</a>
<a name='L402'></a><a href='#L402'>402</a>
<a name='L403'></a><a href='#L403'>403</a>
<a name='L404'></a><a href='#L404'>404</a>
<a name='L405'></a><a href='#L405'>405</a>
<a name='L406'></a><a href='#L406'>406</a>
<a name='L407'></a><a href='#L407'>407</a>
<a name='L408'></a><a href='#L408'>408</a>
<a name='L409'></a><a href='#L409'>409</a>
<a name='L410'></a><a href='#L410'>410</a>
<a name='L411'></a><a href='#L411'>411</a>
<a name='L412'></a><a href='#L412'>412</a>
<a name='L413'></a><a href='#L413'>413</a>
<a name='L414'></a><a href='#L414'>414</a>
<a name='L415'></a><a href='#L415'>415</a>
<a name='L416'></a><a href='#L416'>416</a>
<a name='L417'></a><a href='#L417'>417</a>
<a name='L418'></a><a href='#L418'>418</a>
<a name='L419'></a><a href='#L419'>419</a>
<a name='L420'></a><a href='#L420'>420</a>
<a name='L421'></a><a href='#L421'>421</a>
<a name='L422'></a><a href='#L422'>422</a>
<a name='L423'></a><a href='#L423'>423</a>
<a name='L424'></a><a href='#L424'>424</a>
<a name='L425'></a><a href='#L425'>425</a>
<a name='L426'></a><a href='#L426'>426</a>
<a name='L427'></a><a href='#L427'>427</a>
<a name='L428'></a><a href='#L428'>428</a>
<a name='L429'></a><a href='#L429'>429</a>
<a name='L430'></a><a href='#L430'>430</a>
<a name='L431'></a><a href='#L431'>431</a>
<a name='L432'></a><a href='#L432'>432</a>
<a name='L433'></a><a href='#L433'>433</a>
<a name='L434'></a><a href='#L434'>434</a>
<a name='L435'></a><a href='#L435'>435</a>
<a name='L436'></a><a href='#L436'>436</a>
<a name='L437'></a><a href='#L437'>437</a>
<a name='L438'></a><a href='#L438'>438</a>
<a name='L439'></a><a href='#L439'>439</a>
<a name='L440'></a><a href='#L440'>440</a>
<a name='L441'></a><a href='#L441'>441</a>
<a name='L442'></a><a href='#L442'>442</a>
<a name='L443'></a><a href='#L443'>443</a>
<a name='L444'></a><a href='#L444'>444</a>
<a name='L445'></a><a href='#L445'>445</a>
<a name='L446'></a><a href='#L446'>446</a>
<a name='L447'></a><a href='#L447'>447</a>
<a name='L448'></a><a href='#L448'>448</a>
<a name='L449'></a><a href='#L449'>449</a>
<a name='L450'></a><a href='#L450'>450</a>
<a name='L451'></a><a href='#L451'>451</a>
<a name='L452'></a><a href='#L452'>452</a>
<a name='L453'></a><a href='#L453'>453</a>
<a name='L454'></a><a href='#L454'>454</a>
<a name='L455'></a><a href='#L455'>455</a>
<a name='L456'></a><a href='#L456'>456</a>
<a name='L457'></a><a href='#L457'>457</a>
<a name='L458'></a><a href='#L458'>458</a>
<a name='L459'></a><a href='#L459'>459</a>
<a name='L460'></a><a href='#L460'>460</a>
<a name='L461'></a><a href='#L461'>461</a>
<a name='L462'></a><a href='#L462'>462</a>
<a name='L463'></a><a href='#L463'>463</a>
<a name='L464'></a><a href='#L464'>464</a>
<a name='L465'></a><a href='#L465'>465</a>
<a name='L466'></a><a href='#L466'>466</a>
<a name='L467'></a><a href='#L467'>467</a>
<a name='L468'></a><a href='#L468'>468</a>
<a name='L469'></a><a href='#L469'>469</a>
<a name='L470'></a><a href='#L470'>470</a>
<a name='L471'></a><a href='#L471'>471</a>
<a name='L472'></a><a href='#L472'>472</a>
<a name='L473'></a><a href='#L473'>473</a>
<a name='L474'></a><a href='#L474'>474</a>
<a name='L475'></a><a href='#L475'>475</a>
<a name='L476'></a><a href='#L476'>476</a>
<a name='L477'></a><a href='#L477'>477</a>
<a name='L478'></a><a href='#L478'>478</a>
<a name='L479'></a><a href='#L479'>479</a>
<a name='L480'></a><a href='#L480'>480</a>
<a name='L481'></a><a href='#L481'>481</a>
<a name='L482'></a><a href='#L482'>482</a>
<a name='L483'></a><a href='#L483'>483</a>
<a name='L484'></a><a href='#L484'>484</a>
<a name='L485'></a><a href='#L485'>485</a>
<a name='L486'></a><a href='#L486'>486</a>
<a name='L487'></a><a href='#L487'>487</a>
<a name='L488'></a><a href='#L488'>488</a>
<a name='L489'></a><a href='#L489'>489</a>
<a name='L490'></a><a href='#L490'>490</a>
<a name='L491'></a><a href='#L491'>491</a>
<a name='L492'></a><a href='#L492'>492</a>
<a name='L493'></a><a href='#L493'>493</a>
<a name='L494'></a><a href='#L494'>494</a>
<a name='L495'></a><a href='#L495'>495</a>
<a name='L496'></a><a href='#L496'>496</a>
<a name='L497'></a><a href='#L497'>497</a>
<a name='L498'></a><a href='#L498'>498</a>
<a name='L499'></a><a href='#L499'>499</a>
<a name='L500'></a><a href='#L500'>500</a>
<a name='L501'></a><a href='#L501'>501</a>
<a name='L502'></a><a href='#L502'>502</a>
<a name='L503'></a><a href='#L503'>503</a>
<a name='L504'></a><a href='#L504'>504</a>
<a name='L505'></a><a href='#L505'>505</a>
<a name='L506'></a><a href='#L506'>506</a>
<a name='L507'></a><a href='#L507'>507</a>
<a name='L508'></a><a href='#L508'>508</a>
<a name='L509'></a><a href='#L509'>509</a>
<a name='L510'></a><a href='#L510'>510</a>
<a name='L511'></a><a href='#L511'>511</a>
<a name='L512'></a><a href='#L512'>512</a>
<a name='L513'></a><a href='#L513'>513</a>
<a name='L514'></a><a href='#L514'>514</a>
<a name='L515'></a><a href='#L515'>515</a>
<a name='L516'></a><a href='#L516'>516</a>
<a name='L517'></a><a href='#L517'>517</a>
<a name='L518'></a><a href='#L518'>518</a>
<a name='L519'></a><a href='#L519'>519</a>
<a name='L520'></a><a href='#L520'>520</a>
<a name='L521'></a><a href='#L521'>521</a>
<a name='L522'></a><a href='#L522'>522</a>
<a name='L523'></a><a href='#L523'>523</a>
<a name='L524'></a><a href='#L524'>524</a>
<a name='L525'></a><a href='#L525'>525</a>
<a name='L526'></a><a href='#L526'>526</a>
<a name='L527'></a><a href='#L527'>527</a>
<a name='L528'></a><a href='#L528'>528</a>
<a name='L529'></a><a href='#L529'>529</a>
<a name='L530'></a><a href='#L530'>530</a>
<a name='L531'></a><a href='#L531'>531</a>
<a name='L532'></a><a href='#L532'>532</a>
<a name='L533'></a><a href='#L533'>533</a>
<a name='L534'></a><a href='#L534'>534</a>
<a name='L535'></a><a href='#L535'>535</a>
<a name='L536'></a><a href='#L536'>536</a>
<a name='L537'></a><a href='#L537'>537</a>
<a name='L538'></a><a href='#L538'>538</a>
<a name='L539'></a><a href='#L539'>539</a>
<a name='L540'></a><a href='#L540'>540</a>
<a name='L541'></a><a href='#L541'>541</a>
<a name='L542'></a><a href='#L542'>542</a>
<a name='L543'></a><a href='#L543'>543</a>
<a name='L544'></a><a href='#L544'>544</a>
<a name='L545'></a><a href='#L545'>545</a>
<a name='L546'></a><a href='#L546'>546</a></td><td class="line-coverage quiet"><span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-yes">1x</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span>
<span class="cline-any cline-neutral">&nbsp;</span></td><td class="text"><pre class="prettyprint lang-js">import { LessonData } from "@/types/lesson";
&nbsp;
// Module 8: Bayesian Inference
// Copy and paste module 8 lesson data here
&nbsp;
export const module8Lessons: { [key: string]: LessonData } = {
    "8.1": {
      id: "8.1",
      title: "Introduction to Bayesian Thinking",
      duration: "35-40 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes the Fox emerges from the shadows of statistical inference, his film-noir trench coat gleaming under the streetlight, magnifying glass ready for detective work. Unlike classical statistics that treats parameters as fixed unknowns, Bayesian inference treats them as random variables with probability distributions that update as evidence accumulates. Every clue changes the story.",
        characterMessage: "Welcome to the world of Bayesian detective work! I'm Bayes, and I approach every statistical mystery with a cunning, skeptical eye. In my world, we start with beliefs, gather evidence, and systematically update our conclusions. Every piece of data is a clue that changes what we think we know."
      },
      learningObjectives: [
        "Understand the philosophical difference between Bayesian and frequentist approaches",
        "Recognize parameters as random variables with probability distributions",
        "Apply Bayes' theorem in simple inference contexts",
        "Understand the role of prior beliefs in statistical inference",
        "Connect Bayesian thinking to iterative learning and belief updating"
      ],
      coreConcepts: [
        "Parameters as random variables with distributions",
        "Bayes' theorem: P(|data)  P(data|)  P()",
        "Prior beliefs P() before seeing data",
        "Likelihood P(data|) from observed evidence",
        "Posterior P(|data) updated beliefs after evidence"
      ],
      readContent: "Bayesian inference treats unknown parameters as random variables with probability distributions, contrasting with frequentist approaches that treat parameters as fixed but unknown constants. The foundation is Bayes' theorem: P(|data)  P(data|)  P(), which updates prior beliefs P() with likelihood evidence P(data|) to produce posterior beliefs P(|data). This framework naturally incorporates prior knowledge and quantifies uncertainty about parameters. As new data arrives, today's posterior becomes tomorrow's prior, creating an iterative learning process. Bayesian inference provides a coherent framework for combining evidence with prior knowledge while maintaining full probability distributions over unknown quantities.",
      readAnalogy: "Bayesian inference is like detective work where I start each case with some hunches (priors) based on experience. When I find new evidence (likelihood), I don't throw away my previous knowledge - I combine it with the new clues to update my theory of the case (posterior). Each piece of evidence changes my beliefs systematically, and I always maintain uncertainty about what I don't know for sure.",
      readKeyPoints: [
        "Parameters are random variables with probability distributions, not fixed unknowns",
        "Bayes' theorem: posterior  likelihood  prior combines evidence with beliefs",
        "Iterative learning: today's posterior becomes tomorrow's prior"
      ],
      readDigDeeper: "The subjective interpretation of probability in Bayesian statistics allows incorporation of expert knowledge and personal beliefs, making it particularly valuable in situations with limited data or strong domain expertise. This philosophical difference has practical implications for how we model uncertainty.",
      readWhyMatters: "Medical diagnosis naturally follows Bayesian logic: doctors start with base rate knowledge (priors) and update beliefs based on symptoms and test results. Machine learning uses Bayesian methods for spam filtering, recommendation systems, and natural language processing. Scientific inference increasingly adopts Bayesian approaches for incorporating prior research.",
      seeContent: "Explore the conceptual differences between Bayesian and frequentist approaches, visualize how Bayes' theorem updates beliefs with evidence, and observe the iterative learning process through simple examples.",
      hearContent: "Listen as I explain how Bayesian thinking transforms statistics from rigid hypothesis testing into flexible detective work where every clue systematically updates our understanding of the case!",
      hearAudioUrl: "/audio/8.1.mp3",
      doContent: "Use the Bayesian vs Frequentist Comparison tool, practice with the Bayes' Theorem Calculator for simple scenarios, and experiment with the Belief Updating Simulator showing iterative learning.",
      memoryAids: {
        mantra: "Prior beliefs plus likelihood evidence equals posterior knowledge - that's Bayesian detective reverence! Update and iterate, never dogmatic fate!",
        visual: "Picture Bayes in his detective office, filing cabinet full of prior cases (priors), examining new evidence under his magnifying glass (likelihood), then updating his theory board (posterior) with red string connecting all the clues."
      },
      conceptCheck: {
        question: "A doctor knows 1% of patients have a rare disease (prior). A test is 95% accurate. If a patient tests positive, what's the Bayesian approach to diagnosis?",
        options: [
          "Update the 1% prior probability using Bayes' theorem with the positive test likelihood",
          "Conclude the patient has the disease since the test is 95% accurate",
          "The prior probability is irrelevant once we have test results",
          "Run more tests before making any probability statements"
        ],
        correctAnswer: 0,
        explanation: "Bayesian approach combines prior knowledge (1% base rate) with test evidence (95% accuracy) using Bayes' theorem. The posterior probability will be higher than 1% but much lower than 95% due to the low prior probability of disease."
      },
      realWorldConnection: "Netflix recommendation systems use Bayesian approaches to update user preference models with each viewing choice. Email spam filters learn using Bayesian methods that update word probability associations. Medical AI systems combine symptom likelihoods with disease prevalence priors for diagnosis assistance."
    },
  
    "8.2": {
      id: "8.2",
      title: "Prior, Likelihood, and Posterior",
      duration: "40-45 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes opens his detective case files to reveal the three essential components of every Bayesian investigation: the Prior (what we believed before), the Likelihood (what the evidence tells us), and the Posterior (our updated conclusion). Like any good detective story, each element plays a crucial role in reaching the truth.",
        characterMessage: "Every good detective story has three acts! First, the setup - what I believe going in (prior). Second, the investigation - what the evidence reveals (likelihood). Third, the conclusion - my updated theory combining both (posterior). Master these three, and you master Bayesian reasoning!"
      },
      learningObjectives: [
        "Define and distinguish prior, likelihood, and posterior distributions",
        "Calculate posteriors using Bayes' theorem in simple cases",
        "Understand how different priors affect posterior conclusions",
        "Interpret likelihood functions as evidence summaries",
        "Apply the complete Bayesian workflow to real problems"
      ],
      coreConcepts: [
        "Prior distribution P(): beliefs before data",
        "Likelihood function P(data|): evidence given parameters",
        "Posterior distribution P(|data): updated beliefs",
        "Bayes' theorem calculation workflow",
        "Sensitivity analysis for different priors"
      ],
      readContent: "The prior distribution P() represents our beliefs about parameters before observing data, encoding existing knowledge or assumptions. The likelihood function P(data|) summarizes what the observed data tells us about different parameter values - it's not a probability distribution over  but a function showing how likely the data is for each possible  value. The posterior distribution P(|data) combines prior and likelihood via Bayes' theorem: P(|data) = P(data|)P()/P(data), where P(data) is the normalizing constant. The posterior represents our updated beliefs after seeing the evidence. Different priors can lead to different posteriors, making sensitivity analysis important for understanding how prior assumptions affect conclusions.",
      readAnalogy: "Think of each component like elements in my detective work. The prior is my initial theory about the case based on similar cases I've seen before. The likelihood is what each piece of evidence suggests about different theories - some evidence strongly supports certain suspects, other evidence is ambiguous. The posterior is my final theory that combines my initial hunches with all the evidence I've gathered, weighted appropriately.",
      readKeyPoints: [
        "Prior P(): initial beliefs before data; Likelihood P(data|): evidence summary",
        "Posterior P(|data): updated beliefs combining prior and likelihood via Bayes' theorem",
        "Different priors can lead to different posteriors - sensitivity analysis is important"
      ],
      readDigDeeper: "The likelihood principle states that all information about  contained in the data is captured by the likelihood function. This principle underlies Bayesian inference and distinguishes it from frequentist methods that may depend on unobserved data or experimental design details.",
      readWhyMatters: "Drug discovery uses Bayesian methods where prior knowledge about molecular structures combines with experimental evidence. Climate modeling incorporates prior physical knowledge with observational data. A/B testing increasingly uses Bayesian approaches that naturally incorporate business context as priors.",
      seeContent: "Visualize how priors, likelihoods, and posteriors relate mathematically and graphically, explore how different priors affect posterior conclusions, and observe the complete Bayesian updating process step-by-step.",
      hearContent: "Listen as I walk you through each component of Bayesian reasoning like a detective explaining the three acts of a perfect case - setup, investigation, and resolution!",
      hearAudioUrl: "/audio/8.2.mp3",
      doContent: "Use the Prior-Likelihood-Posterior Calculator with interactive sliders, practice with the Sensitivity Analysis Tool for different priors, and experiment with the Bayesian Workflow Simulator.",
      memoryAids: {
        mantra: "Prior sets the stage, likelihood weighs the evidence, posterior shows the sage! Three components in perfect dance - that's Bayesian inference's stance!",
        visual: "Picture Bayes' detective office with three distinct areas: a filing cabinet of prior cases, an evidence examination table with magnifying glass (likelihood), and a conclusion board where everything comes together (posterior)."
      },
      conceptCheck: {
        question: "You flip a coin 3 times and get HHH. With a uniform prior on p (probability of heads), what does Bayes' theorem tell us?",
        options: [
          "Posterior  p  1, favoring higher values of p but maintaining uncertainty",
          "The coin is definitely biased since three heads is unlikely",
          "The posterior equals the likelihood function exactly",
          "We need more data before making any inferences"
        ],
        correctAnswer: 0,
        explanation: "With uniform prior P(p) = 1 and likelihood P(HHH|p) = p, the posterior P(p|HHH)  p  1 = p. This favors higher values of p but maintains uncertainty across all possible values, showing the coin is likely biased toward heads."
      },
      realWorldConnection: "Financial models combine prior market knowledge with current price data to update investment strategies. Medical diagnosis systems combine disease prevalence priors with symptom likelihoods to calculate diagnostic probabilities. Weather forecasting combines physical model priors with observational data for prediction updates."
    },
  
    "8.3": {
      id: "8.3",
      title: "Conjugate Priors &amp; Beta-Binomial",
      duration: "45-50 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes discovers the mathematical elegance of conjugate priors - special prior distributions that play perfectly with certain likelihoods, creating posteriors in the same family. The Beta-Binomial partnership is like a detective duo that always works in perfect harmony, making calculations elegant and interpretations intuitive.",
        characterMessage: "Some mathematical partnerships are pure genius! Conjugate priors are like perfect detective partners - they work so smoothly with certain likelihoods that the posterior is always in the same family. The Beta-Binomial duo is my favorite example of this mathematical elegance in action!"
      },
      learningObjectives: [
        "Understand conjugate priors as mathematically convenient prior-likelihood pairs",
        "Master the Beta-Binomial conjugate relationship",
        "Interpret Beta distribution parameters as prior successes and failures",
        "Calculate Beta-Binomial posteriors analytically",
        "Recognize other common conjugate pairs and their applications"
      ],
      coreConcepts: [
        "Conjugate prior: prior that yields posterior in same family",
        "Beta-Binomial: Beta(,) prior + Binomial likelihood  Beta posterior",
        "Beta distribution parameters:  (prior successes),  (prior failures)",
        "Posterior updating: Beta(+s, +f) where s,f are observed successes/failures",
        "Other conjugate pairs: Normal-Normal, Gamma-Poisson"
      ],
      readContent: "Conjugate priors create mathematical elegance by producing posteriors in the same distribution family as the prior. For Binomial likelihoods, the Beta distribution is conjugate: if the prior is Beta(,) and we observe s successes in n trials, the posterior is Beta(+s, +n-s). The Beta parameters have intuitive interpretations:  represents prior 'pseudo-successes' and  represents prior 'pseudo-failures,' reflecting our prior equivalent sample size of +. This makes updating trivial - just add observed successes to  and failures to . Other conjugate pairs include Normal-Normal (known variance), Gamma-Poisson, and Dirichlet-Multinomial. Conjugacy enables analytical solutions and provides intuitive parameter interpretations that aid in prior specification.",
      readAnalogy: "Conjugate priors are like having the perfect detective partner who speaks your exact language. When I (Beta prior) team up with Binomial likelihood evidence, we communicate so perfectly that our conclusion (Beta posterior) is expressed in my same language, just with updated parameters. It's like having a conversation where each new piece of evidence simply updates my running tally of successes and failures.",
      readKeyPoints: [
        "Conjugate priors yield posteriors in the same distribution family",
        "Beta-Binomial: Beta(,) + s successes  Beta(+s, +n-s)",
        "Beta parameters:  = prior successes,  = prior failures, + = prior sample size"
      ],
      readDigDeeper: "The exponential family structure underlies conjugacy: when both prior and likelihood are in exponential family form, conjugate priors exist naturally. This mathematical structure explains why certain distribution pairs work together so elegantly.",
      readWhyMatters: "A/B testing platforms use Beta-Binomial updating for real-time conversion rate analysis. Clinical trials use conjugate priors to incorporate historical data about treatment success rates. Quality control uses conjugate updating to monitor defect rates with accumulating evidence.",
      seeContent: "Watch Beta-Binomial updating in action with interactive parameter adjustments, visualize how prior parameters affect posterior conclusions, and explore other conjugate pairs in various applications.",
      hearContent: "Listen as I demonstrate the mathematical poetry of conjugate priors - how Beta and Binomial dance together in perfect harmony to create beautifully simple posterior updating!",
      hearAudioUrl: "/audio/8.3.mp3",
      doContent: "Use the Beta-Binomial Calculator with real-time updating, practice with the Conjugate Prior Explorer for different distribution families, and experiment with the Prior Parameter Interpreter.",
      memoryAids: {
        mantra: "Beta meets Binomial in perfect dance, conjugate priors enhance! Add successes to alpha, failures to beta - posterior updating's perfect theta!",
        visual: "Picture Bayes and his perfect detective partner (Beta and Binomial) working seamlessly together, with each new case (data point) simply updating their shared case file in exactly the same format they've always used."
      },
      conceptCheck: {
        question: "Starting with Beta(2,3) prior for conversion rate, you observe 7 successes in 10 trials. What's the posterior?",
        options: [
          "Beta(9,6):  = 2+7 = 9,  = 3+(10-7) = 6",
          "Beta(7,3): only the successes matter for updating ",
          "Beta(2,10): add the total trials to ",
          "Need to calculate using Bayes' theorem - no simple updating rule"
        ],
        correctAnswer: 0,
        explanation: "Beta-Binomial conjugate updating: Beta(,) + s successes in n trials  Beta(+s, +(n-s)). Here: Beta(2,3) + 7 successes in 10 trials  Beta(2+7, 3+3) = Beta(9,6)."
      },
      realWorldConnection: "Conversion rate optimization platforms like Optimizely use Beta-Binomial updating to provide real-time A/B test results. Medical device companies use conjugate priors to incorporate historical safety data into new clinical trials. Online advertising uses Beta-Binomial models to optimize click-through rates with streaming data."
    },
  
    "8.4": {
      id: "8.4",
      title: "Normal-Normal Conjugacy",
      duration: "40-45 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes encounters another mathematical partnership made in heaven: Normal-Normal conjugacy. When investigating continuous mysteries like heights, weights, or temperatures, the Normal distribution pairs with itself in elegant conjugate harmony, creating posterior updating rules that blend prior knowledge with new evidence in perfectly balanced proportions.",
        characterMessage: "The Normal-Normal conjugate pair is mathematical poetry for continuous data! When my prior beliefs and the evidence are both normally distributed, they combine with perfect mathematical harmony. It's like two expert witnesses giving testimony that blends seamlessly into a unified conclusion!"
      },
      learningObjectives: [
        "Understand Normal-Normal conjugacy for continuous parameter estimation",
        "Calculate Normal posterior means as precision-weighted averages",
        "Interpret how sample size affects the balance between prior and data",
        "Apply Normal-Normal updating to real estimation problems",
        "Understand the role of precision (inverse variance) in Bayesian updating"
      ],
      coreConcepts: [
        "Normal-Normal conjugacy for mean estimation with known variance",
        "Posterior mean as precision-weighted average of prior and sample means",
        "Precision  = 1/ as weight in Bayesian updating",
        "Prior and posterior uncertainty quantification",
        "Limit behavior as sample size increases"
      ],
      readContent: "For Normal likelihoods with known variance , the Normal prior is conjugate. If the prior is N(, ) and we observe sample mean x from n observations, the posterior mean is a precision-weighted average:  = ( + nx)/( + n), where  = 1/ is precision. The posterior variance is 1/( + n), showing uncertainty decreases as we add data. When the prior is weak (large ), the posterior is dominated by data. When the prior is strong (small ), it significantly influences the posterior. As n  , the posterior converges to the sample mean regardless of prior, showing data eventually overwhelms prior beliefs.",
      readAnalogy: "Normal-Normal conjugacy is like having two expert witnesses testify about the same fact. The final conclusion (posterior mean) weighs their testimonies based on their credibility (precision). A very confident expert (high precision) gets more weight than an uncertain one. As more independent evidence accumulates, even a confident prior expert gets gradually outweighed by the mounting data.",
      readKeyPoints: [
        "Posterior mean = precision-weighted average of prior mean and sample mean",
        "Precision  = 1/ determines weight in averaging: more precise gets more influence",
        "Posterior uncertainty decreases as data accumulates: 1/( + n)"
      ],
      readDigDeeper: "The precision parameterization reveals the natural mathematical structure of Normal-Normal conjugacy. Precisions add when combining independent Normal information, making the mathematical relationships transparent and computationally efficient.",
      readWhyMatters: "Manufacturing quality control uses Normal-Normal updating to monitor process means with historical knowledge. Environmental monitoring combines prior climate knowledge with new temperature measurements. Financial modeling blends historical return estimates with current market data using Normal-Normal conjugacy.",
      seeContent: "Visualize how Normal priors and posteriors update with different sample sizes, explore precision-weighted averaging with interactive examples, and observe the balance between prior knowledge and data evidence.",
      hearContent: "Listen as I explain how Normal-Normal conjugacy creates the perfect mathematical blend of prior wisdom and new evidence - like expert testimony weighted by credibility!",
      hearAudioUrl: "/audio/8.4.mp3",
      doContent: "Use the Normal-Normal Calculator with precision-weighted averaging, practice with the Prior Strength Explorer showing weak vs strong priors, and experiment with the Sample Size Impact Simulator.",
      memoryAids: {
        mantra: "Precision weighs the evidence fair, prior and data both declare! More precise gets more say - that's the Normal-Normal way!",
        visual: "Picture Bayes consulting two expert witnesses (prior and data), each holding scales representing their precision/confidence, with the final verdict being a weighted average of their testimonies based on their expertise."
      },
      conceptCheck: {
        question: "Prior: N(100, 10), Sample: n=25, x=105, =15. What happens to the posterior mean as n increases?",
        options: [
          "Posterior mean starts between 100 and 105, approaches 105 as n increases",
          "Posterior mean always equals the sample mean 105",
          "Posterior mean always equals the prior mean 100",
          "Posterior mean depends only on the ratio of variances"
        ],
        correctAnswer: 0,
        explanation: "The posterior mean is a precision-weighted average. Initially it's between the prior mean (100) and sample mean (105). As n increases, the data precision (n/) grows, giving more weight to the sample mean, so the posterior approaches 105."
      },
      realWorldConnection: "Manufacturing uses Normal-Normal updating to monitor production line means while incorporating historical process knowledge. Weather forecasting combines prior climate models with current observations using Normal conjugacy. Laboratory testing blends prior instrument calibration with current measurements for improved accuracy."
    },
  
    "8.5": {
      id: "8.5",
      title: "Posterior Predictive Distribution",
      duration: "40-45 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes shifts focus from understanding the past to predicting the future. The posterior predictive distribution answers the detective's ultimate question: 'Given everything I've learned from this case, what should I expect to see next?' It's not just about estimating parameters - it's about forecasting future observations with full uncertainty quantification.",
        characterMessage: "The real test of any detective theory is prediction! The posterior predictive distribution tells me what to expect in future observations, accounting for both my uncertainty about parameters AND the natural randomness in new data. It's the complete picture for forward-looking inference!"
      },
      learningObjectives: [
        "Understand posterior predictive distribution as future observation forecasts",
        "Distinguish between parameter uncertainty and observation uncertainty",
        "Calculate posterior predictive distributions for conjugate cases",
        "Apply predictive distributions to decision-making scenarios",
        "Interpret predictive intervals for future observations"
      ],
      coreConcepts: [
        "Posterior predictive: P(|y) =  P(|)P(|y)d",
        "Parameter uncertainty vs observation uncertainty",
        "Predictive distributions for Beta-Binomial and Normal-Normal",
        "Predictive intervals vs credible intervals",
        "Applications in forecasting and decision-making"
      ],
      readContent: "The posterior predictive distribution P(|y) describes future observations  given observed data y, integrating over parameter uncertainty: P(|y) =  P(|)P(|y)d. This captures both sources of uncertainty: what we don't know about parameters  (epistemic uncertainty) and natural randomness in observations (aleatoric uncertainty). For Beta-Binomial models, if the posterior is Beta(,), the predictive distribution for future successes in m trials is Beta-Binomial. For Normal-Normal models, the predictive distribution is a t-distribution with location equal to posterior mean and scale reflecting both parameter and observation uncertainty. Predictive intervals are typically wider than credible intervals because they account for future observation variability, not just parameter uncertainty.",
      readAnalogy: "Posterior predictive distribution is like asking 'What should I expect to see in my next case?' It's not just about what I think the true crime rate is (parameter estimation), but what actual crimes I'll observe next month, accounting for both my uncertainty about the true rate AND the natural randomness in criminal behavior. Future predictions are always more uncertain than current parameter estimates.",
      readKeyPoints: [
        "Posterior predictive integrates parameter uncertainty with observation uncertainty",
        "P(|y) =  P(|)P(|y)d averages predictions over all possible parameter values",
        "Predictive intervals are wider than credible intervals due to additional observation uncertainty"
      ],
      readDigDeeper: "The posterior predictive distribution provides the foundation for Bayesian model checking: we can compare observed data to draws from the posterior predictive to assess model adequacy. Systematic discrepancies suggest model misspecification.",
      readWhyMatters: "Sales forecasting uses posterior predictive distributions to predict future revenue with uncertainty quantification. Medical prognosis uses predictive distributions to forecast patient outcomes accounting for diagnostic uncertainty. Supply chain management uses predictive distributions for inventory planning under demand uncertainty.",
      seeContent: "Visualize how posterior predictive distributions incorporate both parameter and observation uncertainty, compare predictive intervals to credible intervals, and explore forecasting applications with real-world examples.",
      hearContent: "Listen as I explain how posterior predictive distributions answer the ultimate detective question - not just 'what happened?' but 'what should I expect to happen next?' with full uncertainty quantification!",
      hearAudioUrl: "/audio/8.5.mp3",
      doContent: "Use the Posterior Predictive Calculator for various models, practice with the Uncertainty Decomposition Tool showing parameter vs observation uncertainty, and experiment with the Forecasting Simulator.",
      memoryAids: {
        mantra: "Parameter plus observation uncertainty combine - posterior predictive shows future's design! Not just what is, but what will be - forecasting with probability!",
        visual: "Picture Bayes looking through a crystal ball that shows not just his best estimate of future events, but a probability cloud representing all possible futures, accounting for both his uncertainty about the true pattern and natural randomness in events."
      },
      conceptCheck: {
        question: "You have a Beta(5,3) posterior for success rate. What's the predictive probability of success in the next single trial?",
        options: [
          "5/(5+3) = 5/8 = 0.625, the posterior mean of the success rate",
          "Always 0.5 since each trial is independent",
          "Need to specify the sample size for the predictive calculation",
          "Cannot calculate without knowing the original data"
        ],
        correctAnswer: 0,
        explanation: "For a single future trial with Beta(,) posterior, the predictive probability of success is the posterior mean: /(+) = 5/(5+3) = 5/8 = 0.625. This integrates over all possible success rates weighted by their posterior probabilities."
      },
      realWorldConnection: "E-commerce companies use posterior predictive distributions to forecast next month's sales with uncertainty bands for inventory planning. Healthcare systems use predictive distributions to forecast patient demand for resource allocation. Weather services use Bayesian predictive distributions for probabilistic forecasting beyond point estimates."
    },
  
    "8.6": {
      id: "8.6",
      title: "MAP vs MLE: Point Estimation Approaches",
      duration: "40-45 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes encounters a fundamental choice in parameter estimation: Maximum A Posteriori (MAP) versus Maximum Likelihood Estimation (MLE). Like choosing between a detective who considers all evidence including background knowledge (MAP) versus one who focuses purely on crime scene evidence (MLE), each approach has its strengths and appropriate applications.",
        characterMessage: "Sometimes we need a single best estimate rather than a full distribution. MAP estimation finds the most probable parameter value given everything we know, while MLE finds the value that best explains just the observed data. Each approach tells a different story about what 'best' means!"
      },
      learningObjectives: [
        "Define MAP as the mode of the posterior distribution",
        "Understand MLE as maximizing the likelihood function",
        "Compare MAP and MLE approaches conceptually and computationally",
        "Recognize when MAP equals MLE (uniform priors)",
        "Apply both approaches to practical estimation problems"
      ],
      coreConcepts: [
        "Maximum A Posteriori (MAP): _MAP = argmax P(|data)",
        "Maximum Likelihood Estimation (MLE): _MLE = argmax P(data|)",
        "Relationship: MAP = MLE when prior is uniform",
        "Regularization interpretation of MAP estimation",
        "Computational approaches for complex posteriors"
      ],
      readContent: "MAP estimation finds the parameter value that maximizes the posterior distribution: _MAP = argmax P(|data)  argmax P(data|)P(). This balances fit to data (likelihood) with prior beliefs. MLE maximizes only the likelihood: _MLE = argmax P(data|), ignoring prior information. When priors are uniform, MAP = MLE since the prior doesn't affect the argmax. MAP can be viewed as regularized MLE, where the prior acts as a regularization term preventing overfitting. For complex posteriors, MAP estimation may require numerical optimization. While both give point estimates, they represent different philosophies: MAP incorporates prior knowledge while MLE relies solely on observed data.",
      readAnalogy: "MAP vs MLE is like two different detective philosophies. The MAP detective considers both the crime scene evidence AND the suspect's background and prior behavior to determine the most likely perpetrator. The MLE detective focuses purely on which suspect best explains the crime scene evidence, ignoring background information. Both can be right depending on how much you trust your background knowledge.",
      readKeyPoints: [
        "MAP maximizes posterior P(|data), incorporating both data and prior knowledge",
        "MLE maximizes likelihood P(data|), using only observed data",
        "MAP = MLE when prior is uniform; MAP acts as regularized MLE otherwise"
      ],
      readDigDeeper: "In machine learning, MAP estimation corresponds to regularized loss functions: L2 regularization comes from Gaussian priors, L1 regularization from Laplace priors. This connection shows how Bayesian priors relate to modern machine learning regularization techniques.",
      readWhyMatters: "Machine learning uses MAP estimation for regularized model fitting, preventing overfitting with prior constraints. Computer vision uses MAP estimation for image reconstruction, balancing data fidelity with smoothness priors. Natural language processing uses MAP estimation for parameter estimation in probabilistic models.",
      seeContent: "Visualize MAP and MLE estimates on posterior distributions, explore how different priors affect MAP estimates, and observe the regularization effect of priors in practical estimation problems.",
      hearContent: "Listen as I explain the detective philosophy behind MAP vs MLE - whether to trust your instincts and background knowledge (MAP) or focus purely on the evidence at hand (MLE)!",
      hearAudioUrl: "/audio/8.6.mp3",
      doContent: "Use the MAP vs MLE Calculator with adjustable priors, practice with the Prior Sensitivity Analyzer, and experiment with the Regularization Visualizer showing MAP as penalized MLE.",
      memoryAids: {
        mantra: "MAP uses all we know, MLE lets data show! Prior plus likelihood for MAP's sight, likelihood alone for MLE's might!",
        visual: "Picture Bayes choosing between two detective approaches: one considering both evidence and background files (MAP), another focusing solely on crime scene evidence (MLE), each appropriate for different investigative philosophies."
      },
      conceptCheck: {
        question: "For Beta(2,2) prior and 3 successes in 5 trials, how do MAP and MLE estimates compare?",
        options: [
          "MAP = 4/6 = 2/3, MLE = 3/5 = 0.6; MAP is pulled toward prior mean of 0.5",
          "MAP = MLE = 3/5 since the data dominates",
          "MAP = 0.5 (prior mean), MLE = 3/5 (sample proportion)",
          "Need numerical optimization to find MAP estimate"
        ],
        correctAnswer: 0,
        explanation: "With Beta(2,2) prior + 3 successes in 5 trials, posterior is Beta(5,4). MAP = mode = (5-1)/(5+4-2) = 4/6 = 2/3. MLE = 3/5 = 0.6. MAP is pulled toward the prior mean (0.5) compared to MLE."
      },
      realWorldConnection: "Deep learning uses MAP estimation with weight decay (L2 regularization) to prevent overfitting in neural networks. Medical imaging uses MAP estimation to reconstruct images from noisy measurements, balancing data fidelity with smoothness priors. Speech recognition uses MAP estimation for parameter estimation in hidden Markov models."
    },
  
    "8.7": {
      id: "8.7",
      title: "Bayes Factors &amp; Model Comparison",
      duration: "45-50 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes faces the ultimate detective challenge: comparing competing theories about the same evidence. Bayes factors provide the mathematical framework for weighing different models against each other, answering questions like 'Which explanation better accounts for what we observed?' It's statistical forensics at its finest.",
        characterMessage: "Now for the ultimate detective skill - comparing competing theories! Bayes factors let me weigh different explanations against each other using the same evidence. It's not just about fitting one model, but determining which of several models best explains what we've observed!"
      },
      learningObjectives: [
        "Define Bayes factors as ratios of marginal likelihoods",
        "Understand Bayes factors as evidence for model comparison",
        "Interpret Bayes factor magnitudes using standard scales",
        "Apply Bayes factors to hypothesis testing scenarios",
        "Recognize advantages and limitations of Bayes factor approaches"
      ],
      coreConcepts: [
        "Bayes factor: BF = P(data|M)/P(data|M)",
        "Marginal likelihood P(data|M) =  P(data|,M)P(|M)d",
        "Evidence interpretation: BF &gt; 10 strong, BF &gt; 100 decisive",
        "Model posterior odds: posterior odds = prior odds  Bayes factor",
        "Automatic Occam's razor through marginal likelihood"
      ],
      readContent: "Bayes factors compare models by taking ratios of their marginal likelihoods: BF = P(data|M)/P(data|M). The marginal likelihood P(data|M) =  P(data|,M)P(|M)d represents the probability of observing the data under model M, averaging over all possible parameter values weighted by their priors. Bayes factors update prior model beliefs: posterior odds = prior odds  Bayes factor. Standard interpretation scales suggest BF &gt; 3 provides substantial evidence, BF &gt; 10 strong evidence, and BF &gt; 100 decisive evidence for one model over another. Bayes factors automatically implement Occam's razor - complex models are penalized unless they provide substantially better fit, since their marginal likelihood must spread probability mass over larger parameter spaces.",
      readAnalogy: "Bayes factors are like having a mathematical jury that weighs competing explanations for the same crime. Each theory (model) gets a score based on how well it predicted what actually happened, accounting for how specific vs vague each theory was beforehand. A theory that makes very specific predictions and gets them right beats a vague theory that could explain anything. It's built-in protection against overly complicated explanations.",
      readKeyPoints: [
        "Bayes factor BF = P(data|M)/P(data|M) compares evidence for competing models",
        "Marginal likelihood averages fit over all parameter values weighted by priors",
        "Automatic Occam's razor: complex models penalized unless substantially better"
      ],
      readDigDeeper: "Computing marginal likelihoods can be challenging, especially for complex models. Modern approaches include harmonic mean estimators, thermodynamic integration, and nested sampling. The choice of priors affects Bayes factors, making sensitivity analysis important.",
      readWhyMatters: "Model selection in machine learning uses Bayes factors to choose between different architectures while avoiding overfitting. Scientific hypothesis testing uses Bayes factors to compare competing theories. Medical diagnosis uses Bayes factors to compare different diagnostic explanations for symptoms.",
      seeContent: "Explore Bayes factor calculations for simple model comparisons, visualize how model complexity affects marginal likelihoods, and observe automatic Occam's razor behavior through examples.",
      hearContent: "Listen as I explain how Bayes factors create the ultimate detective showdown - letting competing theories duke it out mathematically to see which best explains the evidence!",
      hearAudioUrl: "/audio/8.7.mp3",
      doContent: "Use the Bayes Factor Calculator for model comparison scenarios, practice with the Evidence Interpretation Guide, and experiment with the Occam's Razor Demonstrator showing complexity penalties.",
      memoryAids: {
        mantra: "Models compete, Bayes factors judge - which theory won't budge? Evidence ratio tells the tale - best explanation will prevail!",
        visual: "Picture Bayes conducting a courtroom where competing model theories present their cases, with Bayes factors as the mathematical jury determining which explanation better accounts for the observed evidence."
      },
      conceptCheck: {
        question: "Comparing two models, you calculate BF = 15. What does this tell you about the evidence?",
        options: [
          "Strong evidence favoring Model 1 over Model 2 (15:1 evidence ratio)",
          "Model 1 is 15 times more likely to be true than Model 2",
          "Model 1 explains 15% more variance than Model 2",
          "Need additional information to interpret this Bayes factor"
        ],
        correctAnswer: 0,
        explanation: "BF = 15 means the observed data is 15 times more likely under Model 1 than Model 2, providing strong evidence (BF &gt; 10) favoring Model 1. This is about evidence strength, not absolute model probabilities."
      },
      realWorldConnection: "Particle physics uses Bayes factors to compare competing theories about fundamental particles from collision data. Genetics uses Bayes factors to compare different evolutionary models explaining DNA sequence variation. Astronomy uses Bayes factors to compare models of stellar formation and galactic structure."
    },
  
    "8.8": {
      id: "8.8",
      title: "Real-World Applications: Medical Diagnosis",
      duration: "45-50 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes applies his detective skills to one of the most important real-world domains: medical diagnosis. Every symptom is a clue, every test result is evidence, and every diagnosis requires combining prior knowledge about disease prevalence with patient-specific information. It's Bayesian reasoning with life-and-death consequences.",
        characterMessage: "Medical diagnosis is pure Bayesian detective work! Doctors start with base rates (how common is this disease?), observe symptoms (evidence), run tests (more evidence), and systematically update their diagnostic beliefs. Every piece of information changes the probability landscape - it's statistics in service of saving lives!"
      },
      learningObjectives: [
        "Apply Bayesian reasoning to medical diagnostic scenarios",
        "Understand base rates and their critical importance in diagnosis",
        "Calculate diagnostic probabilities using sensitivity and specificity",
        "Recognize common diagnostic fallacies and base rate neglect",
        "Interpret multiple test results using sequential Bayesian updating"
      ],
      coreConcepts: [
        "Base rate: disease prevalence P(Disease) in population",
        "Sensitivity: P(Test+|Disease) = true positive rate",
        "Specificity: P(Test-|No Disease) = true negative rate",
        "Positive predictive value: P(Disease|Test+)",
        "Sequential updating with multiple tests"
      ],
      readContent: "Medical diagnosis exemplifies Bayesian reasoning: P(Disease|Symptoms, Tests)  P(Symptoms, Tests|Disease)  P(Disease). The prior P(Disease) represents base rate prevalence in the relevant population. Test characteristics include sensitivity (true positive rate) and specificity (true negative rate). The posterior probability after a positive test is P(Disease|Test+) = Sensitivity  P(Disease) / [Sensitivity  P(Disease) + (1-Specificity)  (1-P(Disease))]. Base rate neglect - ignoring disease prevalence - leads to systematic overdiagnosis. Multiple tests require sequential updating, with each result modifying the probability for the next test. Rare diseases remain unlikely even after positive tests unless sensitivity is extremely high and specificity near perfect.",
      readAnalogy: "Medical diagnosis is like detective work where I start with 'how common is this type of crime in this neighborhood?' (base rate), then look for evidence like fingerprints (symptoms) and DNA tests (diagnostic tests). Each piece of evidence updates my theory, but I always remember that rare crimes stay rare unless the evidence is overwhelming. A positive test for a rare disease is like finding a common fingerprint pattern - suspicious but not conclusive.",
      readKeyPoints: [
        "Base rates (disease prevalence) critically affect diagnostic probabilities",
        "Positive predictive value depends on sensitivity, specificity, AND base rate",
        "Rare diseases remain unlikely even after positive tests unless tests are nearly perfect"
      ],
      readDigDeeper: "The diagnostic odds ratio (DOR) = (Sensitivity/(1-Sensitivity)) / ((1-Specificity)/Specificity) provides a single measure of test performance that's independent of disease prevalence, facilitating test comparison across different populations.",
      readWhyMatters: "Electronic health records increasingly incorporate Bayesian diagnostic assistance to help doctors avoid base rate neglect and diagnostic errors. Precision medicine uses Bayesian approaches to personalize treatment based on individual risk factors. Public health screening programs use Bayesian analysis to determine optimal testing strategies.",
      seeContent: "Work through medical diagnostic scenarios with interactive probability calculations, visualize how base rates affect diagnostic conclusions, and explore common diagnostic fallacies through realistic examples.",
      hearContent: "Listen as I walk through medical diagnosis like a detective case - starting with base rates, gathering symptomatic evidence, running tests, and systematically updating diagnostic probabilities!",
      hearAudioUrl: "/audio/8.8.mp3",
      doContent: "Use the Medical Diagnosis Calculator with real diagnostic scenarios, practice with the Base Rate Impact Simulator, and experiment with the Sequential Testing Analyzer for multiple test results.",
      memoryAids: {
        mantra: "Base rate first, then test with care - rare stays rare despite positive scare! Sensitivity, specificity, prevalence combine - Bayesian diagnosis, truly divine!",
        visual: "Picture Bayes as a medical detective, consulting case files for base rates (how common is this condition?), examining symptoms as clues, ordering tests as additional evidence, and systematically updating his diagnostic theory with each new piece of information."
      },
      conceptCheck: {
        question: "A disease affects 1 in 1000 people. A test has 95% sensitivity and 95% specificity. What's P(Disease|Positive Test)?",
        options: [
          "About 1.9% - most positive tests are false positives due to low base rate",
          "95% - the test is 95% accurate",
          "90% - sensitivity times specificity",
          "50% - positive test makes disease equally likely as not"
        ],
        correctAnswer: 0,
        explanation: "P(Disease|+) = (0.95  0.001) / [0.95  0.001 + 0.05  0.999] = 0.00095 / 0.050895  0.019 = 1.9%. Low base rate (0.1%) means most positive tests are false positives despite good test characteristics."
      },
      realWorldConnection: "IBM Watson Health uses Bayesian diagnostic reasoning to assist oncologists in cancer diagnosis and treatment planning. COVID-19 testing protocols incorporated Bayesian thinking about base rates in different populations to interpret test results. Genetic counseling uses Bayesian analysis to calculate disease risk probabilities based on family history and genetic test results."
    },
  
    "8.9": {
      id: "8.9",
      title: "Real-World Applications: Spam Filtering",
      duration: "40-45 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes turns his detective skills to the digital realm: email spam filtering. Every word in an email is a clue, and Bayesian classifiers learn from experience which word patterns distinguish legitimate emails from spam. It's machine learning powered by Bayesian reasoning, protecting millions of inboxes daily.",
        characterMessage: "Email spam filtering showcases Bayesian machine learning in action! Every word is evidence, and my job is learning which combinations point to spam versus legitimate email. As new emails arrive, I continuously update my beliefs about what spam looks like. It's adaptive detective work for the digital age!"
      },
      learningObjectives: [
        "Understand naive Bayes classification for text data",
        "Apply Bayesian reasoning to feature-based classification",
        "Recognize the 'naive' independence assumption and its practical effectiveness",
        "Calculate spam probabilities using word frequency evidence",
        "Understand how Bayesian classifiers adapt and learn from new data"
      ],
      coreConcepts: [
        "Naive Bayes classifier: P(Spam|Words)  P(Words|Spam)  P(Spam)",
        "Feature independence assumption: P(Words|Spam) = P(Word_i|Spam)",
        "Word frequency likelihoods from training data",
        "Laplace smoothing for unseen words",
        "Adaptive learning through continuous updating"
      ],
      readContent: "Naive Bayes spam filtering classifies emails using P(Spam|Words)  P(Words|Spam)  P(Spam). The 'naive' assumption treats words as independent given the class: P(Words|Spam) = P(Word_i|Spam), greatly simplifying computation. Word probabilities are estimated from training data: P(Word|Spam) = (count of word in spam + ) / (total words in spam +   vocabulary size), where  provides Laplace smoothing for unseen words. Despite the unrealistic independence assumption, naive Bayes performs remarkably well in practice. The classifier adapts by updating word probabilities as new labeled emails arrive, making it naturally adaptive to evolving spam tactics. Classification involves computing P(Spam|Email) and P(Ham|Email), choosing the class with higher posterior probability.",
      readAnalogy: "Spam filtering is like learning to recognize criminal signatures. Each word is like a behavioral clue - some words appear much more often in spam (like 'FREE!' or 'URGENT!') while others are more common in legitimate email (like names and work terms). Although I naively assume these clues are independent (which isn't really true), this simplification works amazingly well for catching digital criminals!",
      readKeyPoints: [
        "Naive Bayes assumes word independence: P(Words|Class) = P(Word_i|Class)",
        "Word probabilities learned from training data with Laplace smoothing",
        "Continuously adaptive: updates beliefs as new labeled emails arrive"
      ],
      readDigDeeper: "Despite violating the independence assumption (words in emails are clearly dependent), naive Bayes remains effective because it only needs to rank classes correctly, not estimate probabilities accurately. The decision boundary is often robust to assumption violations.",
      readWhyMatters: "Gmail and other email providers use Bayesian-inspired spam filters to protect billions of users daily. Text classification applications extend to sentiment analysis, news categorization, and content moderation. The principles apply broadly to any classification problem with high-dimensional categorical features.",
      seeContent: "Explore spam classification with interactive word probability calculations, visualize how different words contribute evidence for spam vs ham classification, and observe adaptive learning as the classifier sees new examples.",
      hearContent: "Listen as I explain how Bayesian spam filtering turns every email into a detective case, using word patterns as evidence to distinguish digital criminals from legitimate correspondents!",
      hearAudioUrl: "/audio/8.9.mp3",
      doContent: "Use the Spam Classifier Simulator with real email examples, practice with the Word Evidence Analyzer showing probability contributions, and experiment with the Adaptive Learning Demonstrator.",
      memoryAids: {
        mantra: "Words are clues in digital crime, Bayes filters spam every time! Independence naive but works so well - email protection's Bayesian spell!",
        visual: "Picture Bayes examining emails like crime scenes, with each word highlighted as evidence pointing toward spam or legitimate email, continuously updating his criminal profiling database with each new case."
      },
      conceptCheck: {
        question: "An email contains 'FREE' and 'money'. If P(FREE|Spam)=0.8, P(FREE|Ham)=0.1, P(money|Spam)=0.6, P(money|Ham)=0.3, and P(Spam)=0.4, what's P(Spam|FREE,money)?",
        options: [
          "Higher than P(Ham|FREE,money) since spam likelihoods are higher for both words",
          "Exactly 0.4 since that's the prior probability of spam",
          "Cannot calculate without knowing the exact normalization constant",
          "Equal to P(Ham|FREE,money) since we need more evidence"
        ],
        correctAnswer: 0,
        explanation: "P(Spam|Words)  P(FREE|Spam)  P(money|Spam)  P(Spam) = 0.8  0.6  0.4 = 0.192. P(Ham|Words)  0.1  0.3  0.6 = 0.018. Since 0.192 &gt; 0.018, the email is classified as spam."
      },
      realWorldConnection: "Google's Gmail uses sophisticated Bayesian-inspired filters that have evolved beyond simple naive Bayes to protect over 1.5 billion users from spam. Social media platforms use similar techniques for content moderation and fake news detection. Customer service systems use Bayesian classification to route support tickets to appropriate departments."
    },
  
    "8.10": {
      id: "8.10",
      title: "Bayes' Bayesian Inference Mastery Capstone",
      duration: "50-60 minutes",
      characterId: "bayes",
      narrativeHook: {
        story: "Bayes faces his ultimate case: a comprehensive Bayesian investigation that synthesizes every technique in his detective arsenal. From prior specification through posterior analysis, from conjugate updating to model comparison - this capstone demonstrates the complete art of Bayesian reasoning applied to complex, realistic scenarios.",
        characterMessage: "Time for the ultimate Bayesian investigation! This final case brings together every technique we've mastered - prior elicitation, likelihood analysis, posterior updating, predictive distributions, and model comparison. Let's solve a complex mystery that showcases the full power of Bayesian detective work!"
      },
      learningObjectives: [
        "Synthesize all Bayesian concepts in comprehensive real-world investigations",
        "Apply complete Bayesian workflow from prior specification to decision-making",
        "Demonstrate mastery of conjugate updating, model comparison, and prediction",
        "Interpret results with appropriate uncertainty quantification",
        "Connect Bayesian reasoning to practical decision-making under uncertainty"
      ],
      coreConcepts: [
        "Complete Bayesian workflow: prior  likelihood  posterior  prediction",
        "Prior sensitivity analysis and robustness checking",
        "Model comparison using Bayes factors",
        "Decision-making under uncertainty with posterior distributions",
        "Communication of Bayesian results to non-technical audiences"
      ],
      readContent: "This capstone project synthesizes every Bayesian concept into comprehensive real-world investigations. You'll specify informative priors based on domain knowledge, analyze complex likelihood functions, compute posteriors using conjugate relationships, generate predictive distributions for decision-making, and compare competing models using Bayes factors. The project emphasizes the complete Bayesian workflow: starting with careful prior elicitation, incorporating evidence through likelihood analysis, updating beliefs systematically, and using posterior distributions for practical decision-making. You'll also conduct sensitivity analysis to ensure conclusions are robust to prior assumptions and communicate results effectively to stakeholders who need to make decisions under uncertainty.",
      readAnalogy: "This capstone is like Bayes solving the ultimate detective case that requires every skill in his arsenal - from understanding the criminal background (priors) through analyzing all evidence (likelihood) to reaching systematic conclusions (posterior) and predicting future crimes (predictive distributions). It's the complete demonstration of Bayesian detective mastery!",
      readKeyPoints: [
        "Complete Bayesian workflow: prior specification through decision-making applications",
        "Integration of conjugate updating, model comparison, and predictive analysis",
        "Sensitivity analysis and robust conclusions under uncertainty"
      ],
      readDigDeeper: "This project workflow mirrors professional Bayesian analysis in industry and research. Data scientists use these complete workflows for A/B testing, recommendation systems, and predictive modeling. The ability to think coherently about uncertainty and update beliefs systematically distinguishes Bayesian practitioners.",
      readWhyMatters: "This capstone demonstrates career-ready Bayesian inference skills. Tech companies use complete Bayesian workflows for product analytics and machine learning. Pharmaceutical companies apply Bayesian methods for clinical trial design and analysis. Financial firms use Bayesian approaches for risk modeling and algorithmic trading with uncertainty quantification.",
      seeContent: "Work through comprehensive Bayesian analysis workflows integrating all course concepts, visualize complete uncertainty propagation from priors through predictions, and observe how Bayesian reasoning supports principled decision-making under uncertainty.",
      hearContent: "Listen as I guide you through the ultimate demonstration of Bayesian mastery - every concept working together with the cunning precision and skeptical wisdom that defines excellent Bayesian detective work!",
      hearAudioUrl: "/audio/8.10.mp3",
      doContent: "Complete comprehensive Bayesian projects: specify appropriate priors based on domain knowledge, analyze complex evidence through likelihood functions, compute posteriors using conjugate relationships, generate predictive distributions, compare competing models, and provide decision recommendations with full uncertainty quantification.",
      memoryAids: {
        mantra: "Every prior, every update, every Bayesian pursuit! From belief through evidence to decision - that's inference precision!",
        visual: "Picture yourself as Bayes' accomplished detective partner, using every Bayesian technique with cunning precision to solve complex real-world mysteries that require systematic reasoning under uncertainty and principled decision-making."
      },
      conceptCheck: {
        question: "In Part C, you compare three models and find Bayes factors BF = 8.5 and BF = 23.1. What can you conclude about the relative evidence?",
        options: [
          "Strong evidence for Model 1 over Models 2 and 3, with Model 1 vs 3 being most decisive",
          "Model 1 is 8.5 times more likely than Model 2 and 23.1 times more likely than Model 3",
          "Models can be ranked as 1 &gt; 2 &gt; 3 but strength differences are unclear",
          "Need to calculate BF = BF/BF = 2.7 for complete comparison"
        ],
        correctAnswer: 0,
        explanation: "BF = 8.5 provides substantial evidence for Model 1 over Model 2. BF = 23.1 provides strong evidence for Model 1 over Model 3. The evidence most strongly favors Model 1, with the comparison against Model 3 being more decisive than against Model 2."
      },
      realWorldConnection: "This capstone mirrors real Bayesian consulting workflows: pharmaceutical statisticians designing adaptive clinical trials, tech company data scientists building recommendation systems, and financial quants developing risk models. The complete Bayesian reasoning skills you've developed apply directly to careers requiring principled decision-making under uncertainty with systematic belief updating."
    }
  };</pre></td></tr></table></pre>

                <div class='push'></div><!-- for sticky footer -->
            </div><!-- /wrapper -->
            <div class='footer quiet pad2 space-top1 center small'>
                Code coverage generated by
                <a href="https://istanbul.js.org/" target="_blank" rel="noopener noreferrer">istanbul</a>
                at 2025-06-30T10:34:17.195Z
            </div>
        <script src="../../../prettify.js"></script>
        <script>
            window.onload = function () {
                prettyPrint();
            };
        </script>
        <script src="../../../sorter.js"></script>
        <script src="../../../block-navigation.js"></script>
    </body>
</html>
    