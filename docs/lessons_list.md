# Complete Lessons List

Generated on: 2025-07-09T09:19:20.322Z

Total Lessons: 96
Total Interactive Components: 37 (24 completed, 13 remaining)


## Module 1

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 1.1 | Order of Operations & Algebraic Basics | 25 min | ollie | N/A |
| 1.2 | Factoring & Expanding Expressions | 30-35 minutes | ollie | ollie_foundation_builder |
| 1.3 | Linear & Quadratic Equations | 35-40 minutes | ollie | ollie_equation_solver |
| 1.4 | Inequalities & Absolute Values | 25-30 minutes | ollie | N/A |
| 1.5 | Function Notation & Concepts | 30-35 minutes | ollie | N/A |
| 1.6 | Graphing Functions | 40-45 minutes | ollie | ollie_function_grapher |
| 1.7 | Coordinate Geometry Essentials | 25-30 minutes | ollie | N/A |
| 1.8 | Vectors & Greek Symbols Preview | 20-25 minutes | vera | N/A |

## Module 2

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 2.1 | Vector Basics - Arrows with Purpose | 30-35 minutes | vera | vera_vector_playground |
| 2.2 | Vector Addition & Scalar Multiplication | 35-40 minutes | vera | vera_vector_arithmetic |
| 2.3 | The Dot Product - Measuring Similarity | 40-45 minutes | vera | N/A |
| 2.4 | Vector Norms - Measuring Distance | 25-30 minutes | vera | N/A |
| 2.5 | Linear Combinations - Building New Vectors | 35-40 minutes | vera | vera_linear_combination |
| 2.6 | Linear Independence - Fundamental Directions | 35-40 minutes | vera | N/A |
| 2.7 | Basis and Dimension - The Foundation Framework | 40-45 minutes | vera | N/A |
| 2.8 | Vector Spaces - The Abstract Framework | 35-40 minutes | vera | N/A |
| 2.9 | Vera's Forest Mapping Capstone Project | 45-60 minutes | vera | *planned* |

## Module 3

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 3.1 | Matrix Basics - Organizing Information Systematically | 30-35 minutes | max | N/A |
| 3.2 | Matrix Addition & Scalar Multiplication | 25-30 minutes | max | N/A |
| 3.3 | Matrix Multiplication - Systematic Transformation | 40-45 minutes | max | max_matrix_transformer |
| 3.4 | Identity Matrix & Matrix Inverses | 35-40 minutes | max | N/A |
| 3.5 | Determinants & Matrix Properties | 35-40 minutes | max | max_determinant_explorer |
| 3.6 | Elementary Row Operations & Matrix Rank | 40-45 minutes | max | N/A |
| 3.7 | Matrix-Vector Products as Transformations | 40-45 minutes | max | N/A |
| 3.8 | Composition of Linear Maps & Change of Basis | 40-45 minutes | max | N/A |
| 3.9 | Block Matrices & Advanced Organization | 35-40 minutes | max | N/A |
| 3.10 | Max's Data Organization Capstone Project | 50-60 minutes | max | N/A |

## Module 4

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 4.1 | Introduction to Eigenvalues & Eigenvectors | 35-40 minutes | eileen | N/A |
| 4.2 | Finding Eigenvalues - The Characteristic Equation | 40-45 minutes | eileen | eileen_eigenvalue_detective |
| 4.3 | Finding Eigenvectors - Solving the Null Space | 40-45 minutes | eileen | N/A |
| 4.4 | Diagonalization - Revealing Matrix Structure | 45-50 minutes | eileen | eileen_diagonalization_explorer |
| 4.5 | Symmetric Matrices & Orthogonal Diagonalization | 40-45 minutes | eileen | N/A |
| 4.6 | Applications: PCA & Data Analysis | 45-50 minutes | eileen | eileen_pca_dimension_reducer |
| 4.7 | Matrix Powers & Exponentials | 35-40 minutes | eileen | eileen_matrix_powers |
| 4.8 | Complex Eigenvalues & Oscillatory Behavior | 40-45 minutes | eileen | eileen_complex_eigenvalues |
| 4.9 | Eileen's Pattern Discovery Capstone Project | 50-60 minutes | eileen | *planned* |

## Module 5

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 5.1 | Introduction to Multivariable Functions | 30-35 minutes | delta | delta_partial_derivative_explorer |
| 5.2 | Limits in Multivariable Calculus | 40-45 minutes | delta | *planned* |
| 5.3 | Continuity and Surfaces | 35-40 minutes | delta | *planned* |
| 5.4 | Introduction to Partial Derivatives | 40-45 minutes | delta | *planned* |
| 5.5 | Computing Partial Derivatives | 35-40 minutes | delta | N/A |
| 5.6 | The Gradient Vector | 45-50 minutes | delta | delta_gradient_explorer |
| 5.7 | Directional Derivatives | 40-45 minutes | delta | *planned* |
| 5.8 | The Jacobian Matrix | 45-50 minutes | delta | *planned* |
| 5.9 | Applications: Optimization Preview | 40-45 minutes | delta | delta_constrained_optimization |
| 5.10 | Dr. Delta's Multivariable Calculus Capstone | 50-60 minutes | delta | *planned* |

## Module 6

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 6.1 | Introduction to Optimization | 35-40 minutes | greta | N/A |
| 6.2 | Critical Points & The First Derivative Test | 40-45 minutes | greta | N/A |
| 6.3 | The Second Derivative Test & Hessian Matrix | 45-50 minutes | greta | greta_hessian_analyzer |
| 6.4 | Convex vs Non-Convex Functions | 40-45 minutes | greta | N/A |
| 6.5 | Introduction to Gradient Descent | 45-50 minutes | greta | greta_gradient_descent_climber |
| 6.6 | Step Size & Learning Rate Selection | 40-45 minutes | greta | N/A |
| 6.7 | Momentum & Advanced Gradient Methods | 45-50 minutes | greta | *planned* |
| 6.8 | Constrained Optimization Preview | 40-45 minutes | greta | N/A |
| 6.9 | Global vs Local Optimization | 40-45 minutes | greta | N/A |
| 6.10 | Greta's Optimization Mastery Capstone | 50-60 minutes | greta | *planned* |

## Module 7

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 7.1 | Introduction to Probability & Sample Spaces | 35-40 minutes | pippa | N/A |
| 7.2 | Conditional Probability & Independence | 40-45 minutes | pippa | N/A |
| 7.3 | Random Variables & Probability Distributions | 40-45 minutes | pippa | pippa_random_variables |
| 7.4 | Common Discrete Distributions | 45-50 minutes | pippa | pippa_probability_magic |
| 7.5 | Common Continuous Distributions | 45-50 minutes | pippa | N/A |
| 7.6 | Expectation & Variance | 40-45 minutes | pippa | N/A |
| 7.7 | Law of Large Numbers & Central Limit Theorem | 45-50 minutes | pippa | pippa_clt_demonstration |
| 7.8 | PDF vs CDF: Complete Distribution Descriptions | 40-45 minutes | pippa | N/A |
| 7.9 | Sampling & Sampling Variability | 40-45 minutes | pippa | *planned* |
| 7.10 | Pippa's Probability & Distributions Capstone | 50-60 minutes | pippa | *planned* |

## Module 8

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 8.1 | Introduction to Statistical Inference | 35-40 minutes | sigmund | N/A |
| 8.2 | Sampling Distribution & Standard Error | 40-45 minutes | sigmund | N/A |
| 8.3 | Confidence Intervals | 45-50 minutes | sigmund | N/A |
| 8.4 | Introduction to Hypothesis Testing | 45-50 minutes | sigmund | sigmund_hypothesis_arena |
| 8.5 | Test Statistics & P-values | 45-50 minutes | sigmund | N/A |
| 8.6 | t-tests, χ², & ANOVA Preview | 45-50 minutes | sigmund | N/A |
| 8.7 | Type I & Type II Errors | 40-45 minutes | sigmund | N/A |
| 8.8 | Statistical Power | 40-45 minutes | sigmund | N/A |
| 8.9 | Practical vs Statistical Significance | 40-45 minutes | sigmund | N/A |
| 8.10 | Sigmund's Hypothesis Testing Mastery Capstone | 50-60 minutes | sigmund | N/A |

## Module 9

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 9.1 | Introduction to Bayesian Thinking | 35-40 minutes | bayes | N/A |
| 9.2 | Prior, Likelihood, and Posterior | 40-45 minutes | bayes | bayes_medical_diagnosis |
| 9.3 | Conjugate Priors & Beta-Binomial | 45-50 minutes | bayes | N/A |
| 9.4 | Normal-Normal Conjugacy | 40-45 minutes | bayes | N/A |
| 9.5 | Posterior Predictive Distribution | 40-45 minutes | bayes | N/A |
| 9.6 | MAP vs MLE: Point Estimation Approaches | 40-45 minutes | bayes | N/A |
| 9.7 | Bayes Factors & Model Comparison | 45-50 minutes | bayes | N/A |
| 9.8 | Real-World Applications: Medical Diagnosis | 45-50 minutes | bayes | N/A |
| 9.9 | Real-World Applications: Spam Filtering | 40-45 minutes | bayes | N/A |
| 9.10 | Bayes' Bayesian Inference Mastery Capstone | 50-60 minutes | bayes | *planned* |

## Module 10

| Lesson ID | Title | Duration | Character | Interactive Component |
|-----------|-------|----------|-----------|----------------------|
| 10.1 | Capstone Project Overview & Planning | 60-90 minutes | sage | N/A |
| 10.2 | Data Acquisition & Exploratory Analysis | 90-120 minutes | sage | N/A |
| 10.3 | Dimensionality Reduction & Feature Engineering | 90-120 minutes | sage | N/A |
| 10.4 | Model Development & Optimization | 120-150 minutes | sage | N/A |
| 10.5 | Statistical Validation & Inference | 90-120 minutes | sage | sage_data_synthesizer |
| 10.6 | Bayesian Analysis & Decision Making | 90-120 minutes | sage | N/A |
| 10.7 | Results Communication & Visualization | 90-120 minutes | sage | N/A |
| 10.8 | Portfolio Presentation & Documentation | 120-150 minutes | sage | N/A |
| 10.9 | Peer Review & Feedback Integration | 90-120 minutes | sage | N/A |
| 10.10 | Capstone Reflection & Future Learning Path | 60-90 minutes | sage | N/A |


## Detailed Lesson Information

### Lesson 1.1: Order of Operations & Algebraic Basics

- **Character**: ollie
- **Duration**: 25 min
- **Interactive Component**: None

**Core Concepts**:
- Order of operations (PEMDAS/BODMAS)
- Simplifying expressions
- Like terms
- Distributive property

**Learning Objectives**:
- Apply the order of operations (PEMDAS/BODMAS) correctly
- Simplify complex algebraic expressions
- Identify and work with like terms
- Understand the distributive property

---

### Lesson 1.2: Factoring & Expanding Expressions

- **Character**: ollie
- **Duration**: 30-35 minutes
- **Interactive Component**: ollie_foundation_builder

**Core Concepts**:
- Distributive property
- Factoring common terms
- Difference of squares
- Perfect square trinomials
- FOIL method

**Learning Objectives**:
- Apply the distributive property to expand expressions
- Factor out common terms from algebraic expressions
- Recognize and factor difference of squares patterns
- Use the FOIL method for multiplying binomials

---

### Lesson 1.3: Linear & Quadratic Equations

- **Character**: ollie
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Solving linear equations
- Quadratic formula
- Factoring quadratic equations
- Systems of linear equations
- Checking solutions

**Learning Objectives**:
- Solve linear equations with one variable systematically
- Apply the quadratic formula and factoring to solve quadratic equations
- Solve simple systems of linear equations
- Check solutions and identify extraneous solutions

---

### Lesson 1.4: Inequalities & Absolute Values

- **Character**: ollie
- **Duration**: 25-30 minutes
- **Interactive Component**: None

**Core Concepts**:
- Linear inequalities
- Compound inequalities
- Absolute value equations
- Absolute value inequalities
- Interval notation

**Learning Objectives**:
- Solve linear inequalities and represent solutions on number lines
- Work with compound inequalities and intersection/union concepts
- Solve absolute value equations and inequalities
- Use interval notation to express solution sets

---

### Lesson 1.5: Function Notation & Concepts

- **Character**: ollie
- **Duration**: 30-35 minutes
- **Interactive Component**: None

**Core Concepts**:
- Function notation f(x)
- Domain and range
- Evaluating functions
- Function composition
- One-to-one vs many-to-one

**Learning Objectives**:
- Understand function notation f(x) and its meaning
- Identify domain and range of functions
- Evaluate functions at specific values
- Understand function composition basics
- Distinguish between one-to-one and many-to-one functions

---

### Lesson 1.6: Graphing Functions

- **Character**: ollie
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Coordinate plane basics
- Linear functions (y = mx + b)
- Quadratic functions (parabolas)
- Exponential functions
- Graph interpretation

**Learning Objectives**:
- Graph linear functions and identify slope and y-intercept
- Graph quadratic functions and identify vertex and axis of symmetry
- Graph exponential functions and understand growth patterns
- Interpret graphs to understand function behavior
- Connect algebraic and graphical representations

---

### Lesson 1.7: Coordinate Geometry Essentials

- **Character**: ollie
- **Duration**: 25-30 minutes
- **Interactive Component**: None

**Core Concepts**:
- Distance formula
- Midpoint formula
- Slope calculation
- Parallel and perpendicular lines

**Learning Objectives**:
- Apply the distance formula to find distances between points
- Use the midpoint formula to find centers of line segments
- Calculate slope to measure steepness and direction
- Identify parallel and perpendicular line relationships

---

### Lesson 1.8: Vectors & Greek Symbols Preview

- **Character**: vera
- **Duration**: 20-25 minutes
- **Interactive Component**: None

**Core Concepts**:
- Greek alphabet in mathematics
- Basic vector notation
- Sigma (Σ) notation
- Pi (Π) notation
- Mathematical conventions

**Learning Objectives**:
- Recognize and pronounce common Greek symbols used in mathematics
- Understand basic vector notation and representation
- Use sigma (Σ) notation for sums
- Understand pi (Π) notation for products
- Familiarize with mathematical conventions and notation

---

### Lesson 2.1: Vector Basics - Arrows with Purpose

- **Character**: vera
- **Duration**: 30-35 minutes
- **Interactive Component**: vera_vector_playground

**Core Concepts**:
- Vector definition: magnitude + direction
- Vector notation: **v**, v⃗, or [x, y]
- Geometric vs. algebraic representation
- Position vectors vs. displacement vectors
- Zero vector and unit vectors

**Learning Objectives**:
- Define vectors as quantities with both magnitude and direction
- Distinguish between scalars and vectors in real-world contexts
- Use proper vector notation including bold letters and component form
- Calculate vector magnitude using the distance formula
- Identify unit vectors and zero vectors

---

### Lesson 2.2: Vector Addition & Scalar Multiplication

- **Character**: vera
- **Duration**: 35-40 minutes
- **Interactive Component**: vera_vector_arithmetic

**Core Concepts**:
- Vector addition: geometric and algebraic methods
- Commutative property: **u** + **v** = **v** + **u**
- Scalar multiplication: stretching and shrinking
- Negative vectors and subtraction
- Properties of vector operations

**Learning Objectives**:
- Add vectors using both geometric (tip-to-tail) and algebraic methods
- Apply the commutative property of vector addition
- Multiply vectors by scalars and understand the geometric effect
- Understand vector subtraction as addition of negative vectors
- Apply vector operations to solve real-world problems

---

### Lesson 2.3: The Dot Product - Measuring Similarity

- **Character**: vera
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Dot product: **u** · **v** = |**u**||**v**|cos(θ)
- Algebraic formula: **u** · **v** = u₁v₁ + u₂v₂ + u₃v₃
- Geometric interpretation: projection and similarity
- Orthogonal vectors (dot product = 0)
- Properties: commutative, distributive

**Learning Objectives**:
- Calculate dot products using both geometric and algebraic formulas
- Interpret dot product results in terms of vector similarity
- Understand the geometric meaning of dot product as projection
- Identify orthogonal vectors using the dot product
- Apply dot products to real-world similarity problems

---

### Lesson 2.4: Vector Norms - Measuring Distance

- **Character**: vera
- **Duration**: 25-30 minutes
- **Interactive Component**: None

**Core Concepts**:
- L₂ norm (Euclidean): ||**v**|| = √(x² + y² + z²)
- L₁ norm (Manhattan): ||**v**|| = |x| + |y| + |z|
- L∞ norm (Maximum): ||**v**|| = max(|x|, |y|, |z|)
- Unit vectors: ||**u**|| = 1
- Normalizing vectors: **u** = **v**/||**v**||

**Learning Objectives**:
- Calculate the L₂ (Euclidean) norm using the distance formula
- Understand different types of norms: L₁, L₂, and L∞
- Create unit vectors by normalizing any vector
- Compare different distance metrics for practical problems
- Apply norms to measure data similarity and clustering

---

### Lesson 2.5: Linear Combinations - Building New Vectors

- **Character**: vera
- **Duration**: 35-40 minutes
- **Interactive Component**: vera_linear_combination

**Core Concepts**:
- Linear combination: c₁**v₁** + c₂**v₂** + ... + cₙ**vₙ**
- Span of vectors: all possible linear combinations
- Geometric interpretation: filling space
- Coefficients and their meaning
- Examples in 2D and 3D

**Learning Objectives**:
- Express vectors as linear combinations of other vectors
- Understand the span of a set of vectors geometrically
- Calculate linear combinations algebraically
- Identify when a vector can or cannot be expressed as a linear combination
- Visualize how coefficients control the resulting vector

---

### Lesson 2.6: Linear Independence - Fundamental Directions

- **Character**: vera
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Linear independence definition
- Linear dependence: when vectors are redundant
- Testing for independence
- Geometric interpretation
- Maximum independent vectors in n dimensions

**Learning Objectives**:
- Define linear independence and dependence mathematically
- Test sets of vectors for linear independence
- Understand the geometric meaning of independence
- Identify the maximum number of independent vectors in n dimensions
- Recognize when vectors provide genuinely new information

---

### Lesson 2.7: Basis and Dimension - The Foundation Framework

- **Character**: vera
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Basis: linearly independent + spans the space
- Standard basis vectors: **i**, **j**, **k**
- Dimension: number of vectors in any basis
- Coordinates relative to a basis
- Uniqueness of representation

**Learning Objectives**:
- Define a basis as linearly independent vectors that span the space
- Understand how standard basis vectors work in coordinate systems
- Calculate the dimension of a vector space
- Express vectors in terms of different bases
- Recognize the uniqueness of basis representations

---

### Lesson 2.8: Vector Spaces - The Abstract Framework

- **Character**: vera
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Vector space axioms (simplified)
- Examples: ℝ², ℝ³, function spaces, polynomial spaces
- Subspaces: spaces within spaces
- Vector addition and scalar multiplication properties
- Null space and column space (preview)

**Learning Objectives**:
- Understand vector space axioms and their universal applicability
- Recognize examples of vector spaces beyond geometric vectors
- Identify subspaces within larger vector spaces
- Understand how vector space properties enable advanced mathematics
- Connect vector spaces to real-world applications in data science

---

### Lesson 2.9: Vera's Forest Mapping Capstone Project

- **Character**: vera
- **Duration**: 45-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Trail system design using vector addition
- Distance calculations using norms
- Optimal positioning using linear combinations
- Coverage analysis using linear independence
- Coordinate system creation using basis theory

**Learning Objectives**:
- Apply vector addition and scalar multiplication to design trail systems
- Use vector norms and dot products for distance and similarity calculations
- Implement linear combinations and independence for optimal resource placement
- Create coordinate systems using basis and dimension concepts
- Synthesize all vector concepts in a comprehensive real-world project

---

### Lesson 3.1: Matrix Basics - Organizing Information Systematically

- **Character**: max
- **Duration**: 30-35 minutes
- **Interactive Component**: None

**Core Concepts**:
- Matrix definition and structure
- Matrix dimensions (m × n)
- Element notation: A[i,j] or aᵢⱼ
- Special matrices: zero, identity, diagonal
- Square vs rectangular matrices

**Learning Objectives**:
- Define matrices as rectangular arrays of numbers with systematic organization
- Understand matrix dimensions and element addressing using row-column notation
- Distinguish between different types of matrices (square, rectangular, zero, identity)
- Navigate matrix structure and locate specific elements efficiently
- Recognize real-world applications where matrix organization is essential

---

### Lesson 3.2: Matrix Addition & Scalar Multiplication

- **Character**: max
- **Duration**: 25-30 minutes
- **Interactive Component**: None

**Core Concepts**:
- Matrix addition: element-wise operation
- Scalar multiplication: uniform scaling
- Dimension compatibility for addition
- Properties: commutative, associative
- Zero matrix as additive identity

**Learning Objectives**:
- Add matrices by combining corresponding elements systematically
- Multiply matrices by scalars to scale entire datasets uniformly
- Understand when matrix addition is possible (same dimensions required)
- Apply the commutative and associative properties of matrix addition
- Recognize the zero matrix as the additive identity

---

### Lesson 3.3: Matrix Multiplication - Systematic Transformation

- **Character**: max
- **Duration**: 40-45 minutes
- **Interactive Component**: max_matrix_transformer

**Core Concepts**:
- Matrix multiplication algorithm (row × column)
- Dimension compatibility (m×n)(n×p) = (m×p)
- Non-commutative property: AB ≠ BA generally
- Matrix multiplication as transformation
- Applications in systems and graphics

**Learning Objectives**:
- Calculate matrix products using the row-column dot product method
- Understand when matrix multiplication is defined (inner dimensions must match)
- Recognize that matrix multiplication is not commutative
- Interpret matrix multiplication as systematic transformation application
- Apply matrix multiplication to solve systems and perform transformations

---

### Lesson 3.4: Identity Matrix & Matrix Inverses

- **Character**: max
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Identity matrix: I with 1s on diagonal, 0s elsewhere
- Matrix inverse: A⁻¹ where AA⁻¹ = I
- 2×2 inverse formula using determinant
- Singular vs invertible matrices
- Applications in solving Ax = b

**Learning Objectives**:
- Understand the identity matrix as the multiplicative identity
- Calculate 2×2 matrix inverses using the determinant formula
- Determine when matrices are invertible vs singular
- Apply matrix inverses to solve linear equations
- Recognize geometric interpretations of identity and inverse transformations

---

### Lesson 3.5: Determinants & Matrix Properties

- **Character**: max
- **Duration**: 35-40 minutes
- **Interactive Component**: max_determinant_explorer

**Core Concepts**:
- 2×2 determinant: det([[a,b],[c,d]]) = ad - bc
- 3×3 determinant using cofactor expansion
- Geometric interpretation: area/volume scaling
- det(A) = 0 ↔ matrix is singular
- Determinant properties and applications

**Learning Objectives**:
- Calculate determinants for 2×2 and 3×3 matrices
- Interpret determinants as area/volume scaling factors
- Understand the relationship between determinants and matrix invertibility
- Recognize how determinants indicate orientation preservation or reversal
- Apply determinant properties to solve practical problems

---

### Lesson 3.6: Elementary Row Operations & Matrix Rank

- **Character**: max
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Elementary row operations: swap, scale, add
- Row echelon form and reduced row echelon form
- Row operations preserve linear relationships
- Matrix rank as number of independent rows
- Rank and linear independence connection

**Learning Objectives**:
- Apply the three elementary row operations systematically
- Use row operations to transform matrices to row echelon form
- Understand how row operations preserve matrix information while changing appearance
- Calculate matrix rank as the number of linearly independent rows
- Recognize the relationship between rank and linear independence

---

### Lesson 3.7: Matrix-Vector Products as Transformations

- **Character**: max
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Matrix-vector product as transformation
- Matrix columns as transformed basis vectors
- Types: rotation, scaling, reflection, shear, projection
- Composition of transformations
- Linear transformation properties

**Learning Objectives**:
- Interpret matrix-vector multiplication as geometric transformation
- Understand how matrix columns determine where basis vectors go
- Analyze different types of transformations: rotation, scaling, reflection, projection
- Compose transformations by multiplying matrices
- Connect linear transformations to real-world applications

---

### Lesson 3.8: Composition of Linear Maps & Change of Basis

- **Character**: max
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Transformation composition: (BA)(v) = B(A(v))
- Order matters: AB ≠ BA generally
- Change of basis using transition matrices
- Coordinate conversion between bases
- Applications in graphics and data analysis

**Learning Objectives**:
- Compose linear transformations through matrix multiplication
- Understand how transformation order affects the final result
- Perform change of basis using transition matrices
- Convert between different coordinate systems systematically
- Apply composition and basis change to solve complex problems

---

### Lesson 3.9: Block Matrices & Advanced Organization

- **Character**: max
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Block matrix partitioning and structure
- Block addition and multiplication rules
- Special block forms: diagonal, triangular
- Applications in large-scale computation
- Computational efficiency advantages

**Learning Objectives**:
- Understand block matrix structure and notation
- Perform block matrix addition and multiplication
- Recognize when block structure simplifies complex calculations
- Apply block matrices to partition complex problems systematically
- Use block matrices for efficient computation and data organization

---

### Lesson 3.10: Max's Data Organization Capstone Project

- **Character**: max
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Urban data organization using matrix structures
- Traffic flow optimization using matrix operations
- Resource allocation through matrix calculations
- Coordinate transformations for city planning
- Large-scale data management with block matrices

**Learning Objectives**:
- Apply matrix addition and multiplication to combine and transform city data systematically
- Use determinants and inverses to solve urban optimization problems
- Implement matrix transformations for coordinate system conversions in city planning
- Design block matrix structures for large-scale urban data organization
- Synthesize all matrix concepts in a comprehensive real-world project

---

### Lesson 4.1: Introduction to Eigenvalues & Eigenvectors

- **Character**: eileen
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Eigenvector definition: Av = λv where v ≠ 0
- Eigenvalue as scaling factor λ
- Geometric interpretation: preserved directions
- Matrix personality through eigenstructure
- Applications in data analysis and physics

**Learning Objectives**:
- Understand eigenvectors as special directions preserved by matrix transformations
- Define eigenvalues as scaling factors along eigenvector directions
- Recognize the geometric meaning of eigenvalue-eigenvector pairs
- Identify why eigenvalues and eigenvectors reveal matrix structure
- Connect eigenvector concepts to real-world applications

---

### Lesson 4.2: Finding Eigenvalues - The Characteristic Equation

- **Character**: eileen
- **Duration**: 40-45 minutes
- **Interactive Component**: eileen_eigenvalue_detective

**Core Concepts**:
- Characteristic equation: det(A - λI) = 0
- Characteristic polynomial in λ
- Algebraic multiplicity of eigenvalues
- Real vs complex eigenvalues
- Solving quadratic and cubic characteristic equations

**Learning Objectives**:
- Derive the characteristic equation det(A - λI) = 0
- Calculate eigenvalues by solving the characteristic polynomial
- Understand why the determinant equation gives eigenvalues
- Handle 2×2 and simple 3×3 characteristic equations
- Interpret multiple eigenvalues and their geometric significance

---

### Lesson 4.3: Finding Eigenvectors - Solving the Null Space

- **Character**: eileen
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Eigenvector equation: (A - λI)v = 0
- Null space computation using row reduction
- Eigenspace as span of eigenvectors
- Geometric multiplicity vs algebraic multiplicity
- Eigenvector normalization and scaling freedom

**Learning Objectives**:
- Find eigenvectors by solving (A - λI)v = 0 for each eigenvalue λ
- Understand eigenvectors as null space vectors of (A - λI)
- Handle cases with multiple linearly independent eigenvectors
- Normalize eigenvectors and understand scaling freedom
- Recognize geometric vs algebraic multiplicity differences

---

### Lesson 4.4: Diagonalization - Revealing Matrix Structure

- **Character**: eileen
- **Duration**: 45-50 minutes
- **Interactive Component**: eileen_diagonalization_explorer

**Core Concepts**:
- Diagonalization formula: A = PDP⁻¹
- Diagonalizable vs defective matrices
- Eigenvector matrix P and eigenvalue matrix D
- Matrix powers: Aⁿ = PDⁿP⁻¹
- Change of basis interpretation

**Learning Objectives**:
- Understand diagonalization as A = PDP⁻¹ where D is diagonal
- Determine when matrices are diagonalizable vs defective
- Construct diagonalization using eigenvector matrix P and eigenvalue matrix D
- Apply diagonalization to simplify matrix powers and exponentials
- Recognize the geometric meaning of diagonalization as change of basis

---

### Lesson 4.5: Symmetric Matrices & Orthogonal Diagonalization

- **Character**: eileen
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Symmetric matrices: A = Aᵀ
- Real eigenvalues for symmetric matrices
- Orthogonal eigenvectors from different eigenvalues
- Orthogonal diagonalization: A = QDQᵀ
- Spectral theorem and applications

**Learning Objectives**:
- Understand why symmetric matrices have real eigenvalues
- Recognize that symmetric matrices have orthogonal eigenvectors
- Perform orthogonal diagonalization using orthonormal eigenvectors
- Apply the spectral theorem for symmetric matrices
- Connect orthogonal diagonalization to principal component analysis

---

### Lesson 4.6: Applications: PCA & Data Analysis

- **Character**: eileen
- **Duration**: 45-50 minutes
- **Interactive Component**: eileen_pca_dimension_reducer

**Core Concepts**:
- Covariance matrix and its eigenvalue decomposition
- Principal components as eigenvectors
- Explained variance and eigenvalues
- Dimensionality reduction and data compression
- Applications in face recognition, genetics, finance

**Learning Objectives**:
- Understand PCA as eigenvalue decomposition of covariance matrices
- Compute principal components from data using eigenvalue analysis
- Interpret principal components as directions of maximum variance
- Apply dimensionality reduction using top eigenvalues/eigenvectors
- Recognize PCA applications in data science and machine learning

---

### Lesson 4.7: Matrix Powers & Exponentials

- **Character**: eileen
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Matrix powers: Aⁿ = PDⁿP⁻¹
- Matrix exponentials: e^At = Pe^{Dt}P⁻¹
- Stability analysis using eigenvalue magnitudes
- Markov chain steady states
- Population dynamics and growth models

**Learning Objectives**:
- Compute matrix powers efficiently using diagonalization
- Understand matrix exponentials and their eigenvalue computation
- Analyze long-term behavior of discrete dynamical systems
- Apply eigenvalue analysis to Markov chains and population models
- Connect matrix powers to differential equation solutions

---

### Lesson 4.8: Complex Eigenvalues & Oscillatory Behavior

- **Character**: eileen
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complex eigenvalues: λ = a ± bi
- Magnitude |λ| determines growth/decay
- Argument arg(λ) determines rotation frequency
- Complex eigenvectors and real solutions
- Applications to oscillatory systems

**Learning Objectives**:
- Understand complex eigenvalues as indicators of rotational behavior
- Interpret complex eigenvalue magnitude and argument geometrically
- Analyze oscillatory solutions to differential equation systems
- Connect complex eigenvalues to spiraling and periodic motion
- Apply complex eigenvalue analysis to vibration and wave problems

---

### Lesson 4.9: Eileen's Pattern Discovery Capstone Project

- **Character**: eileen
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete eigenvalue analysis workflow
- Principal component extraction and interpretation
- Stability analysis for dynamic systems
- Pattern recognition using eigenstructure
- Dimensionality reduction and data compression

**Learning Objectives**:
- Apply eigenvalue decomposition to analyze real-world datasets
- Use PCA to identify principal patterns and reduce dimensionality
- Analyze system stability using eigenvalue magnitudes
- Interpret complex eigenvalues in dynamic system contexts
- Synthesize all eigenvalue concepts in comprehensive pattern discovery

---

### Lesson 5.1: Introduction to Multivariable Functions

- **Character**: delta
- **Duration**: 30-35 minutes
- **Interactive Component**: delta_partial_derivative_explorer

**Core Concepts**:
- Functions f: ℝⁿ → ℝ with multiple inputs
- Surface representations: z = f(x,y)
- Level curves and contour plots
- Domain and range in higher dimensions
- Examples from physics, economics, and engineering

**Learning Objectives**:
- Understand multivariable functions as mappings from ℝⁿ to ℝ
- Visualize functions of two variables using surfaces and level curves
- Interpret domain and range in multivariable contexts
- Recognize real-world applications of multivariable functions
- Develop intuition for how multiple inputs affect single outputs

---

### Lesson 5.2: Limits in Multivariable Calculus

- **Character**: delta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Limit definition: lim(x,y)→(a,b) f(x,y) = L
- Path independence requirement
- Direct substitution method
- Path-dependent limits and non-existence
- Polar coordinate substitution technique

**Learning Objectives**:
- Understand multivariable limits as approaching from all possible paths
- Recognize why path-independence is required for limit existence
- Compute limits using direct substitution when possible
- Identify when limits don't exist due to path-dependence
- Apply squeeze theorem and polar coordinates for limit evaluation

---

### Lesson 5.3: Continuity and Surfaces

- **Character**: delta
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Continuity definition: lim(x,y)→(a,b) f(x,y) = f(a,b)
- Continuous surfaces without holes or jumps
- Removable vs non-removable discontinuities
- Continuity of composite functions
- Physical interpretation of smoothness

**Learning Objectives**:
- Define continuity for multivariable functions using limits
- Visualize continuity as smooth, unbroken surfaces
- Identify points of discontinuity and their geometric interpretation
- Understand how composition preserves continuity
- Connect continuity to physical modeling requirements

---

### Lesson 5.4: Introduction to Partial Derivatives

- **Character**: delta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Partial derivative definition: ∂f/∂x = lim(h→0) [f(x+h,y) - f(x,y)]/h
- Geometric interpretation as surface slopes
- Computing partials by treating other variables as constants
- Notation: fx, ∂f/∂x, ∂z/∂x
- Physical interpretation as sensitivity analysis

**Learning Objectives**:
- Define partial derivatives as limits of difference quotients
- Understand the geometric interpretation as surface slopes
- Compute partial derivatives using single-variable techniques
- Interpret partial derivatives as rates of change in specific directions
- Apply partial derivative notation and terminology correctly

---

### Lesson 5.5: Computing Partial Derivatives

- **Character**: delta
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Product rule: ∂/∂x[uv] = u(∂v/∂x) + v(∂u/∂x)
- Quotient rule: ∂/∂x[u/v] = [v(∂u/∂x) - u(∂v/∂x)]/v²
- Chain rule for composite functions
- Higher-order partials: ∂²f/∂x², ∂²f/∂x∂y
- Implicit partial differentiation

**Learning Objectives**:
- Apply product rule, quotient rule, and chain rule to partial derivatives
- Handle trigonometric, exponential, and logarithmic functions
- Compute higher-order partial derivatives and mixed partials
- Use implicit differentiation for multivariable functions
- Develop computational fluency with complex expressions

---

### Lesson 5.6: The Gradient Vector

- **Character**: delta
- **Duration**: 45-50 minutes
- **Interactive Component**: delta_gradient_explorer

**Core Concepts**:
- Gradient definition: ∇f = ⟨fx, fy⟩ = ⟨∂f/∂x, ∂f/∂y⟩
- Direction of steepest increase property
- Magnitude ||∇f|| = maximum rate of change
- Perpendicularity to level curves
- Gradient vector fields and flow lines

**Learning Objectives**:
- Define the gradient vector as ∇f = ⟨∂f/∂x, ∂f/∂y⟩
- Understand the gradient as the direction of steepest increase
- Interpret gradient magnitude as the maximum rate of change
- Visualize gradients as vector fields on contour plots
- Apply gradients to optimization and level curve analysis

---

### Lesson 5.7: Directional Derivatives

- **Character**: delta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Directional derivative definition: Dᵤf = lim(h→0) [f(a+hû) - f(a)]/h
- Gradient formula: Dᵤf = ∇f · û where û is unit vector
- Maximum directional derivative in gradient direction
- Zero directional derivative along level curves
- Applications to constraint optimization

**Learning Objectives**:
- Define directional derivatives as limits in specified directions
- Compute directional derivatives using the gradient formula
- Understand the relationship Dᵤf = ∇f · û
- Interpret directional derivatives geometrically and physically
- Apply directional derivatives to optimization and constraint problems

---

### Lesson 5.8: The Jacobian Matrix

- **Character**: delta
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Jacobian matrix J = [∂fᵢ/∂xⱼ] for vector functions F: ℝⁿ → ℝᵐ
- Linear approximation: F(x+h) ≈ F(x) + J(x)h
- Jacobian determinant and area/volume scaling
- Chain rule in matrix form
- Applications to coordinate transformations

**Learning Objectives**:
- Define the Jacobian matrix for vector-valued functions
- Organize partial derivatives systematically in matrix form
- Understand the Jacobian as a linear approximation to transformations
- Compute Jacobian determinants and their geometric significance
- Apply Jacobians to change of variables and transformation analysis

---

### Lesson 5.9: Applications: Optimization Preview

- **Character**: delta
- **Duration**: 40-45 minutes
- **Interactive Component**: delta_constrained_optimization

**Core Concepts**:
- Critical points: ∇f = 0
- Second derivative test using discriminant D
- Hessian matrix H = [∂²f/∂xᵢ∂xⱼ]
- Local maxima, minima, and saddle points
- Applications in machine learning and economics

**Learning Objectives**:
- Find critical points by setting ∇f = 0
- Use the second derivative test for classification
- Understand the role of Hessian matrices in optimization
- Apply optimization to real-world problems
- Connect multivariable calculus to machine learning and economics

---

### Lesson 5.10: Dr. Delta's Multivariable Calculus Capstone

- **Character**: delta
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete multivariable calculus workflow
- Function analysis: domain, continuity, partial derivatives
- Gradient analysis and directional derivatives
- Optimization with critical point classification
- Real-world modeling and interpretation

**Learning Objectives**:
- Synthesize all multivariable calculus concepts in one comprehensive project
- Apply partial derivatives, gradients, and optimization to real scenarios
- Demonstrate computational fluency with complex multivariable functions
- Connect mathematical theory to practical problem-solving
- Prepare for advanced applications in machine learning and engineering

---

### Lesson 6.1: Introduction to Optimization

- **Character**: greta
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Optimization: finding max/min values of functions
- Local vs global extrema
- Objective functions and constraints
- Feasible regions and boundary conditions
- Real-world optimization examples

**Learning Objectives**:
- Understand optimization as finding maximum or minimum function values
- Distinguish between local and global extrema
- Recognize optimization problems in real-world contexts
- Identify the role of gradients in optimization
- Connect optimization to machine learning and engineering applications

---

### Lesson 6.2: Critical Points & The First Derivative Test

- **Character**: greta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Critical points: ∇f = 0 (all partial derivatives zero)
- Necessary condition for interior extrema
- Solving systems: ∂f/∂x = 0 and ∂f/∂y = 0
- Types: maxima, minima, saddle points
- First derivative test limitations

**Learning Objectives**:
- Find critical points by solving ∇f = 0
- Understand why critical points are necessary for extrema
- Apply the first derivative test in multiple variables
- Distinguish between different types of critical points
- Handle systems of equations for critical point finding

---

### Lesson 6.3: The Second Derivative Test & Hessian Matrix

- **Character**: greta
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Hessian matrix: H = [fₓₓ fₓᵧ; fᵧₓ fᵧᵧ]
- Discriminant: D = fₓₓfᵧᵧ - (fₓᵧ)²
- Second derivative test classification
- Eigenvalue interpretation of curvature
- Inconclusive cases when D = 0

**Learning Objectives**:
- Construct the Hessian matrix of second partial derivatives
- Apply the second derivative test using discriminant analysis
- Classify critical points as maxima, minima, or saddle points
- Understand the geometric meaning of the Hessian eigenvalues
- Handle inconclusive cases in the second derivative test

---

### Lesson 6.4: Convex vs Non-Convex Functions

- **Character**: greta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Convex function definition: f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)
- Geometric interpretation: line segments lie above surface
- Second derivative test: fₓₓ ≥ 0, fᵧᵧ ≥ 0, D ≥ 0 for convexity
- Global vs local minima in convex functions
- Non-convex challenges: multiple local minima

**Learning Objectives**:
- Define convex and concave functions mathematically
- Recognize convex functions geometrically and algebraically
- Understand why convex optimization is globally solvable
- Identify convex functions using second derivative tests
- Appreciate the challenges of non-convex optimization

---

### Lesson 6.5: Introduction to Gradient Descent

- **Character**: greta
- **Duration**: 45-50 minutes
- **Interactive Component**: greta_gradient_descent_climber

**Core Concepts**:
- Gradient descent algorithm: xₖ₊₁ = xₖ - α∇f(xₖ)
- Learning rate α controls step size
- Iterative approach to find minima
- Convergence criteria and stopping conditions
- Step-by-step algorithm implementation

**Learning Objectives**:
- Understand gradient descent as an iterative optimization algorithm
- Implement the basic gradient descent update rule
- Choose appropriate step sizes (learning rates)
- Recognize convergence criteria and stopping conditions
- Apply gradient descent to simple optimization problems

---

### Lesson 6.6: Step Size & Learning Rate Selection

- **Character**: greta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Learning rate α effects on convergence
- Too small: slow convergence
- Too large: overshooting and divergence
- Adaptive learning rates and scheduling
- Convergence analysis and oscillation patterns

**Learning Objectives**:
- Understand how learning rate affects convergence behavior
- Recognize signs of learning rates that are too large or too small
- Apply learning rate scheduling and adaptive methods
- Analyze convergence patterns and oscillations
- Choose appropriate learning rates for different problem types

---

### Lesson 6.7: Momentum & Advanced Gradient Methods

- **Character**: greta
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Momentum update: vₖ₊₁ = βvₖ + α∇f(xₖ), xₖ₊₁ = xₖ - vₖ₊₁
- Exponential moving average of gradients
- β parameter controls momentum strength
- Nesterov accelerated gradient improvements
- Modern optimizers: Adam, RMSprop overview

**Learning Objectives**:
- Understand momentum as accumulated gradient information
- Implement momentum-based gradient descent algorithms
- Compare vanilla gradient descent with momentum methods
- Analyze how momentum helps escape local minima and saddle points
- Apply advanced optimizers like Adam and RMSprop conceptually

---

### Lesson 6.8: Constrained Optimization Preview

- **Character**: greta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Constraint types: g(x,y) = 0 (equality), h(x,y) ≤ 0 (inequality)
- Feasible region defined by constraints
- Lagrange multipliers for equality constraints
- KKT conditions for inequality constraints
- Real-world constraint examples

**Learning Objectives**:
- Understand constraints as restrictions on feasible solutions
- Distinguish between equality and inequality constraints
- Introduce Lagrange multipliers conceptually
- Recognize constrained optimization in real applications
- Preview advanced techniques for constraint handling

---

### Lesson 6.9: Global vs Local Optimization

- **Character**: greta
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Local optimum: best in neighborhood
- Global optimum: best over entire domain
- Basin of attraction for local minima
- Global optimization strategies: multi-start, simulated annealing
- Computational complexity of global optimization

**Learning Objectives**:
- Distinguish clearly between local and global optima
- Understand why most algorithms find local optima only
- Recognize strategies for global optimization
- Appreciate the computational challenges of global optimization
- Connect local/global concepts to machine learning applications

---

### Lesson 6.10: Greta's Optimization Mastery Capstone

- **Character**: greta
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete optimization workflow
- Critical point analysis and classification
- Gradient descent implementation with momentum
- Convexity analysis and landscape characterization
- Constraint handling and real-world applications

**Learning Objectives**:
- Synthesize all optimization concepts in one comprehensive project
- Apply multiple optimization techniques to solve complex problems
- Demonstrate computational fluency with gradient descent algorithms
- Analyze optimization landscapes using mathematical and geometric tools
- Connect optimization theory to real-world machine learning applications

---

### Lesson 7.1: Introduction to Probability & Sample Spaces

- **Character**: pippa
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Sample space Ω: set of all possible outcomes
- Events as subsets of sample space
- Probability function P: assigns numbers to events
- Basic probability rules and axioms
- Counting principles for finite sample spaces

**Learning Objectives**:
- Define sample spaces as sets of all possible outcomes
- Understand events as subsets of sample spaces
- Calculate basic probabilities using counting principles
- Apply probability rules: addition, multiplication, complement
- Connect probability to real-world uncertainty and decision-making

---

### Lesson 7.2: Conditional Probability & Independence

- **Character**: pippa
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Conditional probability: P(A|B) = P(A∩B)/P(B)
- Independence: P(A|B) = P(A) when B doesn't affect A
- Multiplication rule for intersections
- Tree diagrams for sequential events
- Law of total probability

**Learning Objectives**:
- Define conditional probability as P(A|B) = P(A∩B)/P(B)
- Understand how conditioning updates probabilities with new information
- Recognize independent events where P(A|B) = P(A)
- Apply multiplication rule: P(A∩B) = P(A|B)P(B)
- Use tree diagrams and contingency tables for complex problems

---

### Lesson 7.3: Random Variables & Probability Distributions

- **Character**: pippa
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Random variable: X: Ω → ℝ maps outcomes to numbers
- Discrete vs continuous random variables
- Probability mass function (PMF): P(X = x)
- Probability density function (PDF): f(x)
- Cumulative distribution function (CDF): F(x) = P(X ≤ x)

**Learning Objectives**:
- Define random variables as functions from outcomes to real numbers
- Distinguish between discrete and continuous random variables
- Understand probability mass functions (PMF) for discrete variables
- Introduce probability density functions (PDF) for continuous variables
- Visualize distributions and interpret their shapes and properties

---

### Lesson 7.4: Common Discrete Distributions

- **Character**: pippa
- **Duration**: 45-50 minutes
- **Interactive Component**: pippa_probability_magic

**Core Concepts**:
- Bernoulli(p): single trial with success probability p
- Binomial(n,p): number of successes in n independent trials
- Poisson(λ): number of rare events in fixed time/space
- Geometric distribution for waiting times
- Parameter interpretation and real-world applications

**Learning Objectives**:
- Understand Bernoulli distribution for single success/failure trials
- Master Binomial distribution for counting successes in n trials
- Apply Poisson distribution for modeling rare events
- Recognize when to use each distribution type
- Calculate probabilities and interpret parameters for each distribution

---

### Lesson 7.5: Common Continuous Distributions

- **Character**: pippa
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Uniform(a,b): constant density f(x) = 1/(b-a) on [a,b]
- Exponential(λ): waiting times, f(x) = λe^(-λx) for x ≥ 0
- Normal(μ,σ²): bell curve, f(x) = (1/σ√2π)e^(-(x-μ)²/2σ²)
- Standard Normal Z ~ N(0,1) and Z-scores
- Integration for probability calculations

**Learning Objectives**:
- Understand Uniform distribution for equally likely ranges
- Master Exponential distribution for waiting times and decay
- Explore Normal distribution and its bell curve properties
- Calculate probabilities using integration and standard tables
- Recognize when continuous distributions apply in real scenarios

---

### Lesson 7.6: Expectation & Variance

- **Character**: pippa
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Expectation: E[X] = Σx·P(X=x) or ∫x·f(x)dx
- Variance: Var(X) = E[(X-μ)²] = E[X²] - (E[X])²
- Standard deviation: σ = √Var(X)
- Linearity: E[aX + b] = aE[X] + b
- Variance properties: Var(aX + b) = a²Var(X)

**Learning Objectives**:
- Define expectation E[X] as the weighted average of outcomes
- Calculate expectations for discrete and continuous distributions
- Understand variance Var(X) = E[(X-μ)²] as spread measurement
- Apply linearity of expectation and variance properties
- Interpret expectation and variance in real-world contexts

---

### Lesson 7.7: Law of Large Numbers & Central Limit Theorem

- **Character**: pippa
- **Duration**: 45-50 minutes
- **Interactive Component**: pippa_clt_demonstration

**Core Concepts**:
- Law of Large Numbers: X̄ₙ → μ as n → ∞
- Central Limit Theorem: (X̄ₙ - μ)/(σ/√n) → N(0,1)
- Sample mean distribution: X̄ₙ ~ N(μ, σ²/n)
- Normal approximation to other distributions
- Sample size requirements for CLT applicability

**Learning Objectives**:
- Understand the Law of Large Numbers as convergence of sample means
- Explore the Central Limit Theorem for sums and averages
- Apply Normal approximation to various distributions
- Calculate probabilities using CLT approximations
- Appreciate why Normal distributions appear everywhere in practice

---

### Lesson 7.8: PDF vs CDF: Complete Distribution Descriptions

- **Character**: pippa
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- PDF f(x): probability density, f(x) = F'(x)
- CDF F(x) = P(X ≤ x): cumulative probability
- Relationship: F(x) = ∫₋∞ˣ f(t)dt
- Percentiles and quantiles from CDF
- Probability calculations using both representations

**Learning Objectives**:
- Understand PDF as probability density for continuous variables
- Master CDF as cumulative probability P(X ≤ x)
- Convert between PDF and CDF using calculus relationships
- Interpret percentiles and quantiles using CDF
- Apply both representations to solve probability problems

---

### Lesson 7.9: Sampling & Sampling Variability

- **Character**: pippa
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Population vs sample distinction
- Parameters (μ, σ) vs statistics (X̄, s)
- Sampling distribution of sample mean
- Standard error: SE = σ/√n
- Sampling bias vs random sampling error

**Learning Objectives**:
- Understand sampling as selecting subsets from populations
- Distinguish between population parameters and sample statistics
- Quantify sampling variability using standard errors
- Apply sampling distributions to make population inferences
- Recognize sources of sampling bias and random variation

---

### Lesson 7.10: Pippa's Probability & Distributions Capstone

- **Character**: pippa
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete probability analysis workflow
- Distribution selection and parameter estimation
- Expectation and variance calculations
- Limit theorem applications
- Sampling theory and inference

**Learning Objectives**:
- Synthesize all probability concepts in one comprehensive project
- Apply multiple distribution types to model real phenomena
- Calculate expectations, variances, and use limit theorems
- Analyze sampling scenarios with proper statistical reasoning
- Connect probability theory to practical decision-making applications

---

### Lesson 8.1: Introduction to Statistical Inference

- **Character**: sigmund
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Population parameters vs sample statistics
- Statistical inference framework
- Sampling distributions as inference foundation
- Uncertainty and variability in conclusions
- Scientific method and statistical evidence

**Learning Objectives**:
- Understand statistical inference as population conclusions from sample data
- Distinguish between descriptive and inferential statistics
- Recognize the role of sampling distributions in inference
- Understand uncertainty quantification in statistical conclusions
- Connect inference to scientific method and decision-making

---

### Lesson 8.2: Sampling Distribution & Standard Error

- **Character**: sigmund
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Sampling distribution: distribution of a statistic across all possible samples
- Standard error: standard deviation of sampling distribution
- Central Limit Theorem for sample means
- Standard error formulas: SE = σ/√n for means
- Relationship to confidence and precision

**Learning Objectives**:
- Understand sampling distributions as distributions of statistics across samples
- Calculate and interpret standard errors for various statistics
- Apply the Central Limit Theorem to sampling distribution analysis
- Distinguish between population standard deviation and standard error
- Use sampling distributions to quantify estimation uncertainty

---

### Lesson 8.3: Confidence Intervals

- **Character**: sigmund
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Confidence interval: X̄ ± margin of error
- Confidence level interpretation
- Critical values from Normal and t-distributions
- Margin of error: ME = t*SE or z*SE
- Factors affecting interval width

**Learning Objectives**:
- Construct confidence intervals for population means
- Interpret confidence level and confidence interval meaning
- Apply t-distribution for unknown population variance
- Understand the relationship between confidence level and interval width
- Recognize factors affecting confidence interval precision

---

### Lesson 8.4: Introduction to Hypothesis Testing

- **Character**: sigmund
- **Duration**: 45-50 minutes
- **Interactive Component**: sigmund_hypothesis_arena

**Core Concepts**:
- Null hypothesis H₀: status quo or no effect
- Alternative hypothesis H₁: claim to be tested
- Type I error: rejecting true H₀ (false positive)
- Type II error: failing to reject false H₀ (false negative)
- Significance level α and testing logic

**Learning Objectives**:
- Formulate null and alternative hypotheses correctly
- Understand the logic and structure of hypothesis testing
- Define Type I and Type II errors in testing context
- Apply hypothesis testing framework to real problems
- Recognize the burden of proof concept in statistical testing

---

### Lesson 8.5: Test Statistics & P-values

- **Character**: sigmund
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Test statistic: standardized measure of evidence against H₀
- P-value: P(observing data this extreme | H₀ true)
- P-value interpretation and decision rules
- One-tailed vs two-tailed tests
- Statistical significance vs practical significance

**Learning Objectives**:
- Calculate test statistics for various hypothesis tests
- Understand p-values as conditional probabilities under H₀
- Interpret p-values correctly in hypothesis testing context
- Apply decision rules using p-values and significance levels
- Recognize common misconceptions about p-value interpretation

---

### Lesson 8.6: t-tests, χ², & ANOVA Preview

- **Character**: sigmund
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- One-sample t-test: t = (X̄ - μ₀)/(s/√n)
- Two-sample t-test for comparing means
- Chi-square test for independence in contingency tables
- Chi-square goodness of fit test
- ANOVA F-test for multiple group comparison

**Learning Objectives**:
- Apply one-sample and two-sample t-tests appropriately
- Understand chi-square tests for independence and goodness of fit
- Preview ANOVA for comparing multiple group means
- Recognize when to use each type of test
- Interpret results from different test families

---

### Lesson 8.7: Type I & Type II Errors

- **Character**: sigmund
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Type I error: reject true H₀, probability = α
- Type II error: fail to reject false H₀, probability = β
- Statistical power: 1 - β, probability of detecting true effect
- Trade-off between α and β
- Factors affecting power: effect size, sample size, α level

**Learning Objectives**:
- Define and distinguish Type I and Type II errors clearly
- Understand the trade-off relationship between error types
- Calculate error probabilities in hypothesis testing contexts
- Recognize factors affecting statistical power
- Apply error analysis to real-world decision scenarios

---

### Lesson 8.8: Statistical Power

- **Character**: sigmund
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Statistical power: P(reject H₀ | H₀ false) = 1 - β
- Power depends on: effect size, sample size, α level, population variance
- Power analysis for sample size planning
- Interpreting negative results in low-power studies
- Cohen's conventions for effect sizes

**Learning Objectives**:
- Define statistical power as the probability of detecting true effects
- Calculate power for various hypothesis testing scenarios
- Understand factors that influence statistical power
- Apply power analysis for sample size determination
- Interpret studies with adequate vs inadequate power

---

### Lesson 8.9: Practical vs Statistical Significance

- **Character**: sigmund
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Statistical significance: p < α (effect is real)
- Practical significance: effect size large enough to matter
- Effect size measures: Cohen's d, eta-squared, correlation
- Confidence intervals for effect size interpretation
- Context-dependent definitions of practical importance

**Learning Objectives**:
- Distinguish clearly between statistical and practical significance
- Understand how large samples can make trivial effects statistically significant
- Apply effect size measures to assess practical importance
- Recognize the limitations of p-value-only interpretations
- Make informed decisions combining statistical and practical considerations

---

### Lesson 8.10: Sigmund's Hypothesis Testing Mastery Capstone

- **Character**: sigmund
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete hypothesis testing workflow
- Test selection and assumption checking
- Error analysis and power considerations
- Effect size interpretation and practical significance
- Principled statistical decision-making

**Learning Objectives**:
- Synthesize all hypothesis testing concepts in comprehensive real-world scenarios
- Apply appropriate tests while considering assumptions and limitations
- Interpret results with full consideration of error types and power
- Distinguish between statistical and practical significance appropriately
- Demonstrate mastery of complete inferential reasoning process

---

### Lesson 9.1: Introduction to Bayesian Thinking

- **Character**: bayes
- **Duration**: 35-40 minutes
- **Interactive Component**: None

**Core Concepts**:
- Parameters as random variables with distributions
- Bayes' theorem: P(θ|data) ∝ P(data|θ) × P(θ)
- Prior beliefs P(θ) before seeing data
- Likelihood P(data|θ) from observed evidence
- Posterior P(θ|data) updated beliefs after evidence

**Learning Objectives**:
- Understand the philosophical difference between Bayesian and frequentist approaches
- Recognize parameters as random variables with probability distributions
- Apply Bayes' theorem in simple inference contexts
- Understand the role of prior beliefs in statistical inference
- Connect Bayesian thinking to iterative learning and belief updating

---

### Lesson 9.2: Prior, Likelihood, and Posterior

- **Character**: bayes
- **Duration**: 40-45 minutes
- **Interactive Component**: bayes_medical_diagnosis

**Core Concepts**:
- Prior distribution P(θ): beliefs before data
- Likelihood function P(data|θ): evidence given parameters
- Posterior distribution P(θ|data): updated beliefs
- Bayes' theorem calculation workflow
- Sensitivity analysis for different priors

**Learning Objectives**:
- Define and distinguish prior, likelihood, and posterior distributions
- Calculate posteriors using Bayes' theorem in simple cases
- Understand how different priors affect posterior conclusions
- Interpret likelihood functions as evidence summaries
- Apply the complete Bayesian workflow to real problems

---

### Lesson 9.3: Conjugate Priors & Beta-Binomial

- **Character**: bayes
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Conjugate prior: prior that yields posterior in same family
- Beta-Binomial: Beta(α,β) prior + Binomial likelihood → Beta posterior
- Beta distribution parameters: α (prior successes), β (prior failures)
- Posterior updating: Beta(α+s, β+f) where s,f are observed successes/failures
- Other conjugate pairs: Normal-Normal, Gamma-Poisson

**Learning Objectives**:
- Understand conjugate priors as mathematically convenient prior-likelihood pairs
- Master the Beta-Binomial conjugate relationship
- Interpret Beta distribution parameters as prior successes and failures
- Calculate Beta-Binomial posteriors analytically
- Recognize other common conjugate pairs and their applications

---

### Lesson 9.4: Normal-Normal Conjugacy

- **Character**: bayes
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Normal-Normal conjugacy for mean estimation with known variance
- Posterior mean as precision-weighted average of prior and sample means
- Precision τ = 1/σ² as weight in Bayesian updating
- Prior and posterior uncertainty quantification
- Limit behavior as sample size increases

**Learning Objectives**:
- Understand Normal-Normal conjugacy for continuous parameter estimation
- Calculate Normal posterior means as precision-weighted averages
- Interpret how sample size affects the balance between prior and data
- Apply Normal-Normal updating to real estimation problems
- Understand the role of precision (inverse variance) in Bayesian updating

---

### Lesson 9.5: Posterior Predictive Distribution

- **Character**: bayes
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Posterior predictive: P(ỹ|y) = ∫ P(ỹ|θ)P(θ|y)dθ
- Parameter uncertainty vs observation uncertainty
- Predictive distributions for Beta-Binomial and Normal-Normal
- Predictive intervals vs credible intervals
- Applications in forecasting and decision-making

**Learning Objectives**:
- Understand posterior predictive distribution as future observation forecasts
- Distinguish between parameter uncertainty and observation uncertainty
- Calculate posterior predictive distributions for conjugate cases
- Apply predictive distributions to decision-making scenarios
- Interpret predictive intervals for future observations

---

### Lesson 9.6: MAP vs MLE: Point Estimation Approaches

- **Character**: bayes
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Maximum A Posteriori (MAP): θ̂_MAP = argmax P(θ|data)
- Maximum Likelihood Estimation (MLE): θ̂_MLE = argmax P(data|θ)
- Relationship: MAP = MLE when prior is uniform
- Regularization interpretation of MAP estimation
- Computational approaches for complex posteriors

**Learning Objectives**:
- Define MAP as the mode of the posterior distribution
- Understand MLE as maximizing the likelihood function
- Compare MAP and MLE approaches conceptually and computationally
- Recognize when MAP equals MLE (uniform priors)
- Apply both approaches to practical estimation problems

---

### Lesson 9.7: Bayes Factors & Model Comparison

- **Character**: bayes
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Bayes factor: BF₁₂ = P(data|M₁)/P(data|M₂)
- Marginal likelihood P(data|M) = ∫ P(data|θ,M)P(θ|M)dθ
- Evidence interpretation: BF > 10 strong, BF > 100 decisive
- Model posterior odds: posterior odds = prior odds × Bayes factor
- Automatic Occam's razor through marginal likelihood

**Learning Objectives**:
- Define Bayes factors as ratios of marginal likelihoods
- Understand Bayes factors as evidence for model comparison
- Interpret Bayes factor magnitudes using standard scales
- Apply Bayes factors to hypothesis testing scenarios
- Recognize advantages and limitations of Bayes factor approaches

---

### Lesson 9.8: Real-World Applications: Medical Diagnosis

- **Character**: bayes
- **Duration**: 45-50 minutes
- **Interactive Component**: None

**Core Concepts**:
- Base rate: disease prevalence P(Disease) in population
- Sensitivity: P(Test+|Disease) = true positive rate
- Specificity: P(Test-|No Disease) = true negative rate
- Positive predictive value: P(Disease|Test+)
- Sequential updating with multiple tests

**Learning Objectives**:
- Apply Bayesian reasoning to medical diagnostic scenarios
- Understand base rates and their critical importance in diagnosis
- Calculate diagnostic probabilities using sensitivity and specificity
- Recognize common diagnostic fallacies and base rate neglect
- Interpret multiple test results using sequential Bayesian updating

---

### Lesson 9.9: Real-World Applications: Spam Filtering

- **Character**: bayes
- **Duration**: 40-45 minutes
- **Interactive Component**: None

**Core Concepts**:
- Naive Bayes classifier: P(Spam|Words) ∝ P(Words|Spam) × P(Spam)
- Feature independence assumption: P(Words|Spam) = ∏P(Word_i|Spam)
- Word frequency likelihoods from training data
- Laplace smoothing for unseen words
- Adaptive learning through continuous updating

**Learning Objectives**:
- Understand naive Bayes classification for text data
- Apply Bayesian reasoning to feature-based classification
- Recognize the 'naive' independence assumption and its practical effectiveness
- Calculate spam probabilities using word frequency evidence
- Understand how Bayesian classifiers adapt and learn from new data

---

### Lesson 9.10: Bayes' Bayesian Inference Mastery Capstone

- **Character**: bayes
- **Duration**: 50-60 minutes
- **Interactive Component**: None

**Core Concepts**:
- Complete Bayesian workflow: prior → likelihood → posterior → prediction
- Prior sensitivity analysis and robustness checking
- Model comparison using Bayes factors
- Decision-making under uncertainty with posterior distributions
- Communication of Bayesian results to non-technical audiences

**Learning Objectives**:
- Synthesize all Bayesian concepts in comprehensive real-world investigations
- Apply complete Bayesian workflow from prior specification to decision-making
- Demonstrate mastery of conjugate updating, model comparison, and prediction
- Interpret results with appropriate uncertainty quantification
- Connect Bayesian reasoning to practical decision-making under uncertainty

---

### Lesson 10.1: Capstone Project Overview & Planning

- **Character**: sage
- **Duration**: 60-90 minutes
- **Interactive Component**: None

**Core Concepts**:
- End-to-end data science workflow
- Project planning and scope definition
- Research question formulation
- Dataset selection and evaluation criteria
- Mathematical technique integration strategy

**Learning Objectives**:
- Understand the complete data science workflow from raw data to insights
- Plan a comprehensive project that integrates multiple mathematical domains
- Select appropriate datasets and define meaningful research questions
- Design a project timeline that demonstrates progressive skill building
- Connect mathematical techniques to real-world business or research impact

---

### Lesson 10.2: Data Acquisition & Exploratory Analysis

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: None

**Core Concepts**:
- Data acquisition from multiple sources (APIs, databases, files)
- Data quality assessment using statistical measures
- Exploratory data analysis with descriptive statistics
- Missing data patterns and imputation strategies
- Outlier detection using statistical and geometric methods

**Learning Objectives**:
- Acquire and assess real-world datasets for quality and completeness
- Apply statistical methods for exploratory data analysis
- Use linear algebra concepts for data manipulation and transformation
- Identify patterns, outliers, and relationships using multiple mathematical perspectives
- Document data understanding and cleaning decisions systematically

---

### Lesson 10.3: Dimensionality Reduction & Feature Engineering

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: None

**Core Concepts**:
- Principal Component Analysis (PCA) using eigenvalue decomposition
- Feature selection methods: statistical tests, correlation analysis, importance scores
- Feature engineering: polynomial features, interactions, domain-specific transformations
- Dimensionality reduction evaluation: explained variance, reconstruction error
- Handling multicollinearity using linear algebra techniques

**Learning Objectives**:
- Apply Principal Component Analysis using eigenvalue decomposition
- Implement feature selection techniques using statistical and linear algebra methods
- Create meaningful derived features using domain knowledge and mathematical insight
- Reduce dimensionality while preserving essential information
- Validate dimensionality reduction effectiveness using multiple criteria

---

### Lesson 10.4: Model Development & Optimization

- **Character**: sage
- **Duration**: 120-150 minutes
- **Interactive Component**: None

**Core Concepts**:
- Linear and logistic regression using optimization
- Gradient descent implementation for parameter estimation
- Regularization techniques: Ridge (L2) and Lasso (L1)
- Cross-validation for model selection and validation
- Model comparison using statistical criteria (AIC, BIC, cross-validation error)

**Learning Objectives**:
- Implement multiple modeling approaches using optimization techniques
- Apply gradient descent and advanced optimization methods for parameter estimation
- Use regularization techniques to prevent overfitting
- Validate model performance using statistical methods
- Compare competing models using mathematical criteria

---

### Lesson 10.5: Statistical Validation & Inference

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: sage_data_synthesizer

**Core Concepts**:
- Hypothesis testing for model validation
- Confidence intervals for predictions and parameters
- Bootstrap methods for uncertainty quantification
- Statistical significance testing for model comparison
- Power analysis for determining required sample sizes

**Learning Objectives**:
- Apply hypothesis testing to validate model performance claims
- Construct confidence intervals for model predictions and parameters
- Quantify prediction uncertainty using probabilistic methods
- Compare model performance using statistical significance tests
- Communicate statistical results with appropriate uncertainty acknowledgment

---

### Lesson 10.6: Bayesian Analysis & Decision Making

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: None

**Core Concepts**:
- Bayesian updating of model parameters and predictions
- Prior specification using domain knowledge
- Posterior predictive distributions for decision-making
- Bayesian model averaging and ensemble methods
- Decision theory under uncertainty

**Learning Objectives**:
- Apply Bayesian inference to update model beliefs with new evidence
- Incorporate prior knowledge into analysis using appropriate prior distributions
- Make decisions under uncertainty using Bayesian decision theory
- Implement Bayesian model averaging for robust predictions
- Communicate Bayesian results for business decision-making

---

### Lesson 10.7: Results Communication & Visualization

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: None

**Core Concepts**:
- Data visualization principles for mathematical results
- Audience-appropriate communication strategies
- Narrative structure for analytical presentations
- Business impact quantification and ROI analysis
- Uncertainty communication and limitation acknowledgment

**Learning Objectives**:
- Create effective visualizations that communicate mathematical insights clearly
- Adapt technical depth appropriately for different audience types
- Structure analytical narratives that drive decision-making
- Quantify business impact using mathematical analysis results
- Present uncertainty and limitations honestly while maintaining credibility

---

### Lesson 10.8: Portfolio Presentation & Documentation

- **Character**: sage
- **Duration**: 120-150 minutes
- **Interactive Component**: None

**Core Concepts**:
- Comprehensive project documentation standards
- Mathematical concept integration demonstration
- Professional portfolio development
- Business value articulation and ROI quantification
- Reflective learning and growth planning

**Learning Objectives**:
- Document the complete analytical workflow from problem to solution
- Demonstrate integration of mathematical concepts across all modules
- Create professional-quality deliverables suitable for career portfolios
- Articulate the business value created through mathematical analysis
- Reflect on learning journey and identify areas for continued growth

---

### Lesson 10.9: Peer Review & Feedback Integration

- **Character**: sage
- **Duration**: 90-120 minutes
- **Interactive Component**: None

**Core Concepts**:
- Structured peer review methodology
- Mathematical analysis evaluation criteria
- Constructive feedback delivery and integration
- Collaborative improvement processes
- Professional quality assurance standards

**Learning Objectives**:
- Conduct thorough peer reviews using structured evaluation criteria
- Provide constructive feedback that improves analytical quality
- Receive and integrate feedback to strengthen project outcomes
- Apply professional review standards to mathematical analysis
- Develop collaborative skills essential for data science careers

---

### Lesson 10.10: Capstone Reflection & Future Learning Path

- **Character**: sage
- **Duration**: 60-90 minutes
- **Interactive Component**: None

**Core Concepts**:
- Comprehensive learning reflection and assessment
- Strength identification and growth area analysis
- Advanced learning pathway planning
- Career connection and opportunity identification
- Continuous learning mindset development

**Learning Objectives**:
- Reflect systematically on learning achievements across all mathematical domains
- Identify personal strengths and areas for continued development
- Create a personalized learning plan for advanced mathematical topics
- Connect capstone experience to career goals and opportunities
- Celebrate transformation from mathematical student to practitioner

---

